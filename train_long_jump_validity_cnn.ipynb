{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XXE7LwSXrWh",
        "outputId": "24021f60-fe9e-4d8a-aa55-5d9e3347309e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[INFO] grupos válidos: 83  |  no válidos: 19\n",
            "\n",
            "VALIDOS:\n",
            "  Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-48-46 -> 258 frames (etiqueta=0)\n",
            "  Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-48-46_result -> 258 frames (etiqueta=0)\n",
            "  Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-49-07 -> 202 frames (etiqueta=0)\n",
            "\n",
            "NO VALIDOS:\n",
            "  vn1 -> 189 frames (etiqueta=1)\n",
            "  vn2 -> 253 frames (etiqueta=1)\n",
            "  vn3 -> 229 frames (etiqueta=1)\n"
          ]
        }
      ],
      "source": [
        "# ==== MONTAJE Y RUTAS ====\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "RAIZ = \"/content/drive/My Drive/videoscombi\"\n",
        "RUTA_VALIDOS    = f\"{RAIZ}/videosvalidos\"\n",
        "RUTA_NOVALIDOS  = f\"{RAIZ}/videosnovalidos\"\n",
        "MODELO_SALIDA   = \"/content/drive/My Drive/modelo_saltos_novalidos_seq.h5\"\n",
        "\n",
        "# ==== UTILS ====\n",
        "import os, re, cv2, random, numpy as np, tensorflow as tf\n",
        "random.seed(42); np.random.seed(42); tf.random.set_seed(42) #para hacerlo determinante se fijan las semillas hasta los shuffle.\n",
        "\n",
        "#parametros base de la red neuronal, para determinar como se van a procesar los datos.\n",
        "\n",
        "IMG_SIZE          = (160, 160) #tamaño al que se redimensiona cada frame.\n",
        "FRAMES_POR_VIDEO  = 24 #cuantos frames equiespaciados representan el salto\n",
        "BATCH_SIZE        = 8 # lote de entrenamiento\n",
        "VAL_SPLIT         = 0.2 # fracción de validación.\n",
        "\n",
        "def natkey(s): return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", str(s))] #para tener orden natural, digit si son numeros ok, split para hacerlos int.\n",
        "def es_imagen(p): return str(p).lower().endswith((\".jpg\",\".jpeg\",\".png\")) #reconoce extensiones de imagenes.\n",
        "\n",
        "def listar_subcarpetas(carpeta): #recorre subcarpetas porque son un salto individual\n",
        "    try:\n",
        "        subs = [os.path.join(carpeta,d) for d in os.listdir(carpeta) #Devuelve todos los nombres de archivos y carpetas dentro de \"carpeta\"\n",
        "                if os.path.isdir(os.path.join(carpeta,d))] #pura subcarpeta y join para unir carpeta con subcarpeta y tener la ruta.\n",
        "    except FileNotFoundError: #regresa una lista vacia en lugar de dar error.\n",
        "        return []\n",
        "    return sorted(subs, key=natkey) #ordena las subcarpetas con natkey.\n",
        "\n",
        "def listar_frames_en_carpeta(carpeta): #lista las imagenes que sirven en una carpeta e ignora archivos que no sean imagenes y overlays.\n",
        "    if not os.path.isdir(carpeta): return [] #si no exiaste la carpeta, regresa una lista vacia\n",
        "    fs = []\n",
        "    for f in os.listdir(carpeta): #recorre todos los nombres dentro de la carpeta.\n",
        "        if not es_imagen(f): continue #asegura que sean imagenes.\n",
        "        # ignorar frames con overlays/resultados.\n",
        "        low = f.lower()\n",
        "        if (\"resultado\" in low) or (\"_result\" in low) or (\"overlay\" in low):\n",
        "            continue\n",
        "        p = os.path.join(carpeta, f) #ruta del archivo\n",
        "        if os.path.isfile(p):\n",
        "            fs.append(p) #solo añade si realemnte hay archivo.\n",
        "    return sorted(fs, key=natkey) #devuelve la lista de rutas de imágenes, ordenadas naturalmente con el natkey.\n",
        "\n",
        "#sirve para resumir cada salto completo en una muestra representativa y genera una nueva lista con los frames seleccionados según los índices.\n",
        "\n",
        "def muestrear_k(items, k): #rutas de frames y elementos que se quieren obtener de forma equidistante\n",
        "    if not items: return [] # para verificar que item no este vacio, si lo esta regresa lista vacia.\n",
        "    if len(items) <= k: # si imtem ≤ k repite elementos evita errores.\n",
        "        idx = np.linspace(0, len(items)-1, num=k, dtype=int) #0 primer indice, len es el último válido, numk porque k posiciones, int para enteros, porque el lins hace floats\n",
        "        return [items[i] for i in idx] # de items recorre todo idx, construye una nueva lista y la devuelve.\n",
        "\n",
        "\n",
        "#Esta función prepara las imagenes, convierte los archivos de imagen en tensores numéricos normalizados en el formato y rango que MobileNetV2 necesita.\n",
        "\n",
        "def leer_redimensionar(fp): #fp es la ruta del archivo\n",
        "    im = cv2.imread(fp) #con open cv lee imagenes y regresa un array de numpy con valores de cada pixel\n",
        "    if im is None: return None #si no se puede leer la imagen, regresa none, evita errores.\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) #open cv solo lee rgb por eso se pasa de bgr y tambien TF/K.\n",
        "    im = cv2.resize(im, IMG_SIZE) #redimensiona la imagen al de IMG_SIZE, para que todas las fotos tengan mismo tamaño, es requisito.\n",
        "    # Preprocess de MobileNetV2\n",
        "    im = tf.keras.applications.mobilenet_v2.preprocess_input(im.astype(np.float32)) #pasa las imagenes a float32 por TF luego MNV2, escala pixeles desde (0 a 255) y el rabgo de -1 a 1 porque MNV2 lo necesita asi\n",
        "    return im # devuelve la imagen en RGB tamaño (160, 160) y normalizada para MobileNetV2.\n",
        "\n",
        "#organiza el dataset agrupando los frames pertenecientes a cada salto dentro de su clase correspondiente (“válido” o “no válido”).\n",
        "\n",
        "def recolectar_grupos_de_clase(ruta_clase, etiqueta): #etiqueta es número de clase asociado 0 para válidos 1 para noválidos.\n",
        "    grupos = [] #aca se guardan los conjuntos de frames encontrados.\n",
        "    if not os.path.isdir(ruta_clase): return grupos #verifica si la ruta existe y es carpeta si no, regresa lista vacia.\n",
        "    # subcarpetas\n",
        "    for sub in listar_subcarpetas(ruta_clase): #recorre las subcarpetas de ruta_clase y cada una representa un salto.\n",
        "        fs = listar_frames_en_carpeta(sub) #funcion auxiliar que devuelve las imagenes dentro de la subcarpeta con fs una lista de rutas de imágen.\n",
        "        if fs: grupos.append((fs, etiqueta, sub)) # si fs tiene frames, crea una tupla al listado de grupos, lista frames, numero de clase (0,1) y donde estan esos frames.\n",
        "    # frames directo en la raíz\n",
        "    fs_root = listar_frames_en_carpeta(ruta_clase) # y subcarpetas igual.por si acaso para que funcione aunque no esten en subcarpetas (si lo estan, es para hacer el trabajo más general)\n",
        "    if fs_root: grupos.append((fs_root, etiqueta, ruta_clase)) #si en la carpeta hay frames los agrega como un grupo más, en el mismo formato de tupla.\n",
        "    return grupos # devuelve la lista de grupos encontrados y cada elemento de \"grupos\" tiene el mismo formato de tupla. Lista de frames (fs) Etiqueta de clase (0 o 1) Ruta de origen (sub o ruta_clase)\n",
        "\n",
        "# ==== CHEQUEO DURO DEL DATASET ====\n",
        "gr_validos   = recolectar_grupos_de_clase(RUTA_VALIDOS, 0) #llama a la función con la carpeta de validos (0), regresa una lista de frames para los asosciados a 0.\n",
        "gr_novalidos = recolectar_grupos_de_clase(RUTA_NOVALIDOS, 1) #hace lo mismo pero para inválidos.\n",
        "print(f\"[INFO] grupos válidos: {len(gr_validos)}  |  no válidos: {len(gr_novalidos)}\") #muestra cuantos se encuentran en cada clase y cuántos grupos (subcarpetas o conjuntos de frames) se encontraron en la carpeta de válidos, lo mismo para inválidos.\n",
        "\n",
        "# Muestra 3 grupos por clase con #frames (asegura que NO aparezca 0)\n",
        "for titulo, grupos in [(\"VALIDOS\", gr_validos), (\"NO VALIDOS\", gr_novalidos)]: #itera string que son titulos y las tuplas los grupos.\n",
        "    print(f\"\\n{titulo}:\") #imprime los nombres\n",
        "    for fs, y, path in grupos[:3]: #este segudo bluce es para verificar, recorre los 3 primeros elementos de la lista de grupos\n",
        "        print(f\"  {os.path.basename(path)} -> {len(fs)} frames (etiqueta={y})\") #verifica si hay frames en cada grupo, la etiqueta esta bien y si los grupos estan recolentados.\n",
        "\n",
        "# Chequeo crítico: aborta si no hay datos\n",
        "assert len(gr_validos) + len(gr_novalidos) > 0, \"No encontré grupos con frames. Verifica rutas.\" #suma los lens y esa suma > 0 asi se ve si hay grupo con frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "iKfqvAMbYLb7",
        "outputId": "dde53a13-350a-4bf6-ade4-8d83d2e8c087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] grupos válidos: 83 | no válidos: 19\n",
            "Train V/NV: 67 16\n",
            "Val   V/NV: 16 3\n",
            "[INFO] train videos: 83 | val videos: 19\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TEST] Overfit rápido a 4 videos…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc mini: 1.0 auc mini: 1.0\n",
            "Epoch 1/6\n",
            "     21/Unknown \u001b[1m1102s\u001b[0m 2s/step - accuracy: 0.6109 - auc: 0.4231 - loss: 0.9907"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1352s\u001b[0m 14s/step - accuracy: 0.6039 - auc: 0.4233 - loss: 0.9895 - val_accuracy: 0.5263 - val_auc: 0.6146 - val_loss: 0.7076\n",
            "Epoch 2/6\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6092 - auc: 0.6574 - loss: 0.7460"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 3s/step - accuracy: 0.6128 - auc: 0.6600 - loss: 0.7408 - val_accuracy: 0.5789 - val_auc: 0.6354 - val_loss: 0.6101\n",
            "Epoch 3/6\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7139 - auc: 0.7514 - loss: 0.6091"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 3s/step - accuracy: 0.7143 - auc: 0.7516 - loss: 0.6078 - val_accuracy: 0.6842 - val_auc: 0.6875 - val_loss: 0.5113\n",
            "Epoch 4/6\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7255 - auc: 0.7266 - loss: 0.6301"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 3s/step - accuracy: 0.7259 - auc: 0.7286 - loss: 0.6279 - val_accuracy: 0.5789 - val_auc: 0.7188 - val_loss: 0.6583\n",
            "Epoch 5/6\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6466 - auc: 0.7105 - loss: 0.6049"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 3s/step - accuracy: 0.6517 - auc: 0.7163 - loss: 0.6001 - val_accuracy: 0.7368 - val_auc: 0.7917 - val_loss: 0.4778\n",
            "Epoch 6/6\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7511 - auc: 0.7664 - loss: 0.5005"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 3s/step - accuracy: 0.7492 - auc: 0.7666 - loss: 0.5043 - val_accuracy: 0.5789 - val_auc: 0.8125 - val_loss: 0.7526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Modelo guardado en: /content/drive/My Drive/modelo_saltos_novalidos_seq.h5\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "#  ENTRENAR CLASIFICADOR \"NO VÁLIDO\" (secuencias de frames)\n",
        "#  TODO-EN-UNO  — versión con correcciones A–E\n",
        "#    (A) preprocess_input de MobileNetV2\n",
        "#    (B) orden natural y filtro de overlays\n",
        "#    (C) gen_mini sin doble lectura\n",
        "#    (D) class_weight simétrico (inverso a la frecuencia)\n",
        "#    (E) callbacks: ModelCheckpoint + EarlyStopping\n",
        "# =========================================================\n",
        "import os, re, cv2, random, numpy as np, tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# ----------- CONFIG -----------\n",
        "RAIZ = \"/content/drive/My Drive/videoscombi\" #aca estan las 2 subcarpetas, válidos y no válidos\n",
        "RUTA_VALIDOS   = os.path.join(RAIZ, \"videosvalidos\")\n",
        "RUTA_NOVALIDOS = os.path.join(RAIZ, \"videosnovalidos\") #con join para que la unión funcione bien en todo sistema operativo.\n",
        "\n",
        "IMG_SIZE = (160, 160)        # tamaña al que se redimenciona cada frame (puedes subir a (192,192) si tienes GPU rápida)\n",
        "FRAMES_POR_VIDEO = 24        # el salto se reduce a 24 frames (baja a 12 si quieres velocidad o sube si tienes GPU rápida)\n",
        "BATCH_SIZE = 4               # lote por entrenamiento, en cada paso de entrenamiento hay 4 secuencias de video (baja a 2 si la RAM es poca pero tardará más)\n",
        "VAL_SPLIT = 0.2              # fracción de datos usados para validación el otro .8 es para entrenar.\n",
        "EPOCHS    = 6                # las veces que el modelo verá todo el dataset mientras entrena (baja para más velocidad o sube para mejorar resultados con precaución al sobreentrenamiento.)\n",
        "SEED      = 42               # valor clasico para la semilla de aleatoriedad es para que los resultados sean reproducibles en todo, o sea muestro y shuffle.\n",
        "MODELO_SALIDA = \"/content/drive/My Drive/modelo_saltos_novalidos_seq.h5\" #guardo en .h5 para que se guarde sin reentrenar, ahorra tiempos.\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED) #fijo la semilla de aleatoriedad a 3 librerias la estandar de py, operaciones vectoriales y para inicialización de pesos y mezclado de datos es importante para depurar.\n",
        "\n",
        "# ----------- utilidades de IO -----------\n",
        "def listar_subcarpetas(p): #recorre el directorio p y devuelve las subcarpeta que tiene.\n",
        "    try:\n",
        "        return sorted([os.path.join(p,d) for d in os.listdir(p) if os.path.isdir(os.path.join(p,d))]) #sorted para que este ordenado alfabéticamente mas no natural, eso es de natkey.\n",
        "    except FileNotFoundError: # regresa una lista vacia para no tirar error.\n",
        "        return [] #es para encontrar todas las carpetas con los saltos.\n",
        "\n",
        "def natkey(s):  # (B) orden natural para respetar secuencia temporal\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", str(s))] #split para separar la cadena s en su parte númerica y de texto.\n",
        "                                                                                      #convierte las partes númericas a int y el txt a minusculas asi ya tienen un orden natural de esta forma \"frame1.jpg\"\n",
        "def listar_frames_en_carpeta(p): #recorre p y recoge puras fotos.\n",
        "    # (B) filtro de overlays/resultados y orden natural\n",
        "    exts = (\".jpg\",\".jpeg\",\".png\") #a esto me refiero con puras fotos.\n",
        "    ban  = (\"resultado\", \"_result\", \"overlay\") ]#filtra este tipo de nombres para evitar imagenes marcadas o generadas de intentos anteriores.\n",
        "    try:\n",
        "        fs = [] #se inicia una lista vacia llamada fs y ahi se va a guardar las rutas completas de imagenes válidas que estan en p.\n",
        "        for f in os.listdir(p): #nombres de archivos y carpetas dentro de p. f es la variable no ruta de los elementos tipo frame 1.jpg, overlay_frame1.png, etc.\n",
        "            low = f.lower() #pasa el nombre a minuscula para que las comprobaciones ean más sencillas.\n",
        "            if not low.endswith(exts): continue # si no termina con nombre de foto se la salta (exts = (\".jpg\",\".jpeg\",\".png\"))\n",
        "            if any(b in low for b in ban): continue #si el nombre del archivo tiene la palabra, resultado, _result, overlay, se salta.\n",
        "            fs.append(os.path.join(p,f)) #si pasa los filtros pasa a fs y os.path.join (p,f)construye la ruta.\n",
        "        return sorted(fs, key=natkey) #ordena los archivos por número de frame. #cuando se revisan todos los datos se regresa fs pero ordenada en buen orden, o sea que frame 1, frame 2 y no frame 1, frame 10 como seria alfabeticamente.\n",
        "    except FileNotFoundError: #si p no existe regresa lista vacia, para que no se detenga el programa.\n",
        "        return []\n",
        "\n",
        "def armar_grupos(ruta, etiqueta): #define la funcion que construye grupos, con ruta un str (videos válidos) y etiqueta un int (0)\n",
        "    grupos = [] #cada elemnto es una tupla de la forma :(fs, etiqueta, ruta_grupo)\n",
        "    for sub in listar_subcarpetas(ruta): #revisa subcarpetas de ruta, porque cada subcarpeta es un video\n",
        "        fs = listar_frames_en_carpeta(sub) #todas las imagenes validas dentro de sub, filtra por extensión, excluye \"resultado, _result, overlay\" y ordena de forma natural.\n",
        "        if fs: #para robustez, agrega al grupo si hay al menos un frame, esto evita guardar vacios.\n",
        "            grupos.append((fs, etiqueta, sub)) #agrega a un nuevo grupo que tiene fs validos, etiquetas constantes y el sub como ruta para saber de donde salen los grupos.\n",
        "    fs_root = listar_frames_en_carpeta(ruta)  # este bloque es por si no hya subcarpetas y los frames estan en raíz, fs_root: lista de frames válidos directamente en la raíz de la clase.\n",
        "    if fs_root: # si no esta vacia, se añade un nuevo grupo con su \"ruta_grupo\" como la propia ruta, asi se tiene cubierto.\n",
        "        grupos.append((fs_root, etiqueta, ruta))\n",
        "    return grupos #devuelve la lista complta de grupos con buen fs, etiqueta y ruta.\n",
        "\n",
        "gr_validos   = armar_grupos(RUTA_VALIDOS,   0) #se llama a la función armar_grupos y esta es la ruta del disco con videos válidos.\n",
        "gr_novalidos = armar_grupos(RUTA_NOVALIDOS, 1) #lo mismo pero para no válidos. y las 2 son tuplas.\n",
        "\n",
        "print(f\"[INFO] grupos válidos: {len(gr_validos)} | no válidos: {len(gr_novalidos)}\") #para verificar si la carga del dataset es correcta y cuantos grupos de válidos y no se encuentran, no frames, si subcarpetas.\n",
        "if (len(gr_validos)+len(gr_novalidos)) == 0: #para no seguir entrenando si no hay datos, como antes usma los 2 grupos y si es 0, no encontro ningún grupo.\n",
        "    raise RuntimeError(\"No hay frames en las carpetas. Verifica RAIZ y que existan .jpg/.png.\") #si es el caso lanza este mensaje en consola.\n",
        "\n",
        "# ----------- muestreo y preprocesado -----------\n",
        "def muestrear_k(fs, k): #para seleccionar k frames de una lista de ruta de frames, fs es la lista y k los k frames equiespaciados.\n",
        "    if not fs: return []#si fs esta vacio, devuelve una lista vacía.\n",
        "    if len(fs) <= k: #si la cantidad de frames es ≤ k hay un problema.\n",
        "        return (fs + [fs[-1]]*(k-len(fs)))[:k] #pero se soluciona asi. agarra la losta fs completa y se añaden copias del ultimo frame hasta llegar a k.\n",
        "    idx = np.linspace(0, len(fs)-1, num=k, dtype=int) #el linspace es para equispaciar y si hay más agarra entre 0 y último, o sea: len(fs)=100, k=10 idx = [0, 11, 22, 33, 44, 55, 66, 77, 88, 99]\n",
        "    return [fs[i] for i in idx] #devuelve la lista calculada con idx fs[i] es una ruta a un archivo de imagen, esto nos da las k rutas de frames equispaciadas uniformemente.\n",
        "\n",
        "#este es una función que pasa archivos del disco a un tensor listo para pasar por MobileNetv2\n",
        "\n",
        "def leer_redimensionar(fp, size=IMG_SIZE): #ruta del archivo de imagen y el tamaño al que se redimensiona.\n",
        "    #la función oficial de MobileNetv2 para no hacer un [0,1] dividiendo por 255 preprocess_input de MobileNetV2 ([-1,1]) es la arquitectura que necesita porque se entreno con esa normalización.\n",
        "    img = cv2.imread(fp) #usa opencv para leer la imagen de la ruta fp el resultado es un array de numpy de la forma: (alto, ancho, 3)\n",
        "    if img is None: return None #si la lectura falla por le motivo que sea regresa un none. ayuda a que el pipeline vea si es none para saltarla.\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #pasa de BRG a RGB impprtante porque el opencv esta en bgr y tensor y keras necesita rgb.\n",
        "    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA) # redimensiona al tamaña 160, 160. inter_area es para reducior imagenes manteniendo calidad y evita aliasing, ya con la misma forma en imagenes se puede entrenar.\n",
        "    img = img.astype(np.float32) #pasa el array a float32 porque asi lo trabaja tensorflow no float64 y el preprocess espera floats no enteros de 0-255.\n",
        "\n",
        "    return preprocess_input(img) # fnción de MobileNetv2 es de [-1 a 1] o sea si tenemos pixel 0 seria -1.0, Pixel 127 → ~ -0.0039 Pixel 255 → +1.0\n",
        "\n",
        "# ----------- split estratificado ----------- # es la que se encarga de dividir el data set en entrenamiento y validación.\n",
        "def split_estratificado(V, NV, val_split=0.2, seed=SEED): #validos, no, validación, para que la división sea reproducible.\n",
        "    random.seed(seed) #fijo la semilla para que sea reproducible.\n",
        "    V = V.copy(); NV = NV.copy() # hace copias de las listas originales para no modificarlas, shuffle no altera las listas externas que se usen más adelante.\n",
        "    random.shuffle(V); random.shuffle(NV) #mezcla los elementos de manera aleatoria, cada clase por separado que da estratificación.\n",
        "    nV_val  = max(1, int(len(V)*val_split)) if len(V)>0 else 0 # número proporcional de videos válidos para validación. el if es que si la lista esta vacia regrese 0 de una. y el val es para ver cuantos elementos se van a validación dependiendo la proporción.\n",
        "    nNV_val = max(1, int(len(NV)*val_split)) if len(NV)>0 else 0 #lo mismo pero para no válidos. int redondea enteros para abajo, max es para asegurar que hay elementos y al menos uno pase a validación.\n",
        "    items_val = V[:nV_val] + NV[:nNV_val] # los primeros nV y nNV se van a validación.\n",
        "    items_tr  = V[nV_val:] + NV[nNV_val:] # los demas se quedan en entrenamiento.\n",
        "    random.shuffle(items_val); random.shuffle(items_tr) #mezcla conjunto de validación y de entrene para que no queden agrupados en clase, o sea yodos los válidos primero, rompe el orden inicial, pero sigue en balance.\n",
        "    return items_tr, items_val #regresa grupos de entrenamiento y grupos de validación. (se sigue conservando el mismo formato listaframes, etiqueta y ruta)\n",
        "\n",
        "items_tr, items_val = split_estratificado(gr_validos, gr_novalidos, VAL_SPLIT) #llama al split y válido tiene forma de (frames, 0, ruta) y noválido (frames, 1, ruta) y el val pues el .2.\n",
        "print(\"Train V/NV:\", sum(1 for _,y,_ in items_tr if y==0), sum(1 for _,y,_ in items_tr if y==1)) # se recorre cada tupla(fs,label,ruta) _ ignora, y label, if 0 para válido y el sum es para sumar elementos que cumplen la misma condición.\n",
        "print(\"Val   V/NV:\", sum(1 for _,y,_ in items_val if y==0), sum(1 for _,y,_ in items_val if y==1)) #lo mismo pero para validación, con el objetico de que es balanceable los V y NV en train y val.\n",
        "\n",
        "# ----------- generadores + tf.data -----------\n",
        "def gen(items_lista, k=FRAMES_POR_VIDEO): #reproduce en formato stream, muy importante porque uso yield, tambien porque asi funciona mejor TensorFlow, no uses return.\n",
        "    for fs, y, _ in items_lista: #itera sobre cada grupo y desempaqueta los usa el fs, y, ruta. (no se usan)\n",
        "        sel = muestrear_k(fs, k) #selecciona k rutas de fs (lista de rutas de imagenes equispaciadas) Si len(fs) > k: muestreo uniforme con np.linspace. si len(fs) <= k: rellena repitiendo el último frame hasta llegar a k, asi se garantiza que todas tengan los 24.\n",
        "        batch = [] #acumula los k frames ya leídos y procesados (no solo las rutas)\n",
        "        for fp in sel: #\n",
        "            arr = leer_redimensionar(fp) #lee la imagen y pasa de bgr a rgb, redimenciona a (160 x 160), pasa a float 32, normaliza con preprocess_input de MNV2 de [-1, 1]\n",
        "            if arr is not None: si falla se omite\n",
        "                batch.append(arr) #si sale bien se añade el tensor de la imagen a batch. aqui el batch tiene tensores de entre 0 y k y de forma (H, W, 3)\n",
        "        if not batch: # si no se puede leer ningun frame, salta el grupo y pasa al siguiente. eso para no romper el np.stack con ejemplos vacíos.\n",
        "            continue\n",
        "        while len(batch) < k:  # rellena si faltó alguno\n",
        "            batch.append(batch[-1]) # con el último frame válido.\n",
        "        x = np.stack(batch, axis=0)   #aplica la lista batch en un solo array 4D de forma (K,H,W,3) (imagenes, largo, ancho, color)\n",
        "        yield x.astype(np.float32), np.int32(y) #x es la frecuencia de k frames ya normalizados y float 32, y es etiqueta ya en int32 y como es un gen carga todo 1 a 1, no todos, ideal porque usamos tf.data.Dataset.from_generator.\n",
        "\n",
        "spec_x = tf.TensorSpec(shape=(FRAMES_POR_VIDEO, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32) # el spec es para decirle a TF que forma tienen mis tensores y sus datos (24,160,160,3)\n",
        "spec_y = tf.TensorSpec(shape=(), dtype=tf.int32) # shape para tensor escalar, no tiene dimensiones, solo 1 valor, o sea solo un número no la lista, y ese int porque es compatible con las etiquetas (0 y 1)\n",
        "\n",
        "\n",
        "\n",
        "#se crea un dataset con generator que ve, 1 a 1 los (x,y) que el gen item va yieldando, lamba es para fijar item del DS asi tensorflow lo puede recrear cuando haga falta.\n",
        "#output describe forma y tipo de lo que emite el gen para X el tensor (k,h,w,3) con float32 para Y el de () con int32\n",
        "#shuffle barajea con un buffer de 256 y seed lo hace reproducible, batch pasa x a (BATCH_SIZE,K,H,W,3) y con Y (BATCH_SIZE,)\n",
        "#prefetch hace un pipeline cuando la GPU entrene con batch n la cpu prepara el batch n+1. y el autotune es para que tf elija el tamaño del  prefetch según el hardware.\n",
        "ds_tr  = tf.data.Dataset.from_generator(lambda: gen(items_tr),  output_signature=(spec_x, spec_y)) \\\n",
        "                        .shuffle(256, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "ds_val = tf.data.Dataset.from_generator(lambda: gen(items_val), output_signature=(spec_x, spec_y)) \\ #lo mismo sin shuffle pero si con batch + prefetch, para rendimiento en la validación.\n",
        "                        .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"[INFO] train videos: {len(items_tr)} | val videos: {len(items_val)}\") #cuántos saltos quedaron en que grupo despues del split que es la separación de mi dataset en entrenamiento y validación y es para entrenar y evaluar. para que no memorice la red.\n",
        "\n",
        "# ----------- modelo -----------\n",
        "#la función que construye el modelo de deep learning.\n",
        "def construir_modelo(img_size=IMG_SIZE, k=FRAMES_POR_VIDEO, base_trainable=False, lr=1e-3): #(si es true entrena los pesos de MNV2 si es false se queda congelado, learning rate, que tan rapido aprende el optimizador adam)\n",
        "    inp = layers.Input(shape=(k, img_size[0], img_size[1], 3)) #capa de entrada (k, alto, ancho. color) (24,160,160,3)\n",
        "    base = tf.keras.applications.MobileNetV2(include_top=False, input_shape=img_size+(3,), weights=\"imagenet\") #(no incluye la ultima capa de clasificación de IN, cada imagen es (160,160,3), incia con pesos preentrenados)\n",
        "    base.trainable = base_trainable # controla si MobileNet entrena o no, false se congela y no modifica sus pesos, true, modifica algunos pesos bueno para fine tuning porque tengo muchos datos, ya tendriamos un mapa de características cnn 2D.\n",
        "    x = layers.TimeDistributed(base)(inp) #aplica MobileNet independientemente a cada frame,m decirle a keras pon MN  cada frame indepedientemente usando los mismos pesos.\n",
        "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x) # arriba (batch, k, 5, 5, 1280) ahora (batch, k, 1280) promedia en el espacio.\n",
        "\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x) #promedia en el tiempo, haciedo ya un vector de 1D (batch, k, 1280) con k el tiempo.\n",
        "\n",
        "    x = layers.Dropout(0.3)(x) #apaga aleatoriamente el 30% de valores para que la red sea más independiente,(batch, 1280) sin k porque ya no es frame a frame es el resumen de un video.\n",
        "    out = layers.Dense(1, activation=\"sigmoid\")(x) # prob. de inválido, multiplica por una matriz (1280,1) y suma sesgo, y por el activador es de 0 a 1. de .5 para arriba es mas prob que sea inválido.\n",
        "    model = models.Model(inp, out) #conecta entradas(inp) con salidas(out) por tf.keras.Model representa toda la red, ya esta lista, para predecir y entrenar.\n",
        "    model.compile( #para decirle a keras como entrenar el modelo.\n",
        "        optimizer=tf.keras.optimizers.Adam(lr), # adam con el 1e-3=.001 es mi favorito.\n",
        "        loss=\"binary_crossentropy\", # es válido o inválido, 1 o 0, por eso uso esa función de pérdida.\n",
        "        metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")] #accuracyes el % de predicción correcta , calcula area bajo la curva AUC-ROC (que tan bien distingue entre valores positivos y negativos)\n",
        "    ) #cierra model.compile\n",
        "    return model #ya esta el modelo (Input → MobileNetV2 → GlobalPooling → Dense(sigmoid))\n",
        "\n",
        "model = construir_modelo(base_trainable=False, lr=1e-3) #llamo a construir_modelo(...), base_trainable... se usa para congelar caracteristicas por el MNV2, lo que resulte se queda en model, es para guardar basicamente.\n",
        "model.summary() #resumen tabular cada capa con su nombre\n",
        "\n",
        "#Input: una secuencia de 24 frames de tamaño 160×160×3.\n",
        "#MobileNetV2 (congelada): extrae mapas de características (5×5×1280).\n",
        "#GlobalAveragePooling2D: convierte cada frame en un vector de 1280.\n",
        "#GlobalAveragePooling1D: condensa la secuencia en un solo vector de 1280.\n",
        "#Dropout: regulariza.\n",
        "#Dense(1, sigmoid): devuelve una probabilidad de “inválido”.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------- (Opcional) overfit mini — sanity check -----------\n",
        "#para sabr si el pipeline de datos funciona, ver si la red puede aprender\n",
        "mini = gr_validos[:2] + gr_novalidos[:2] #toma los 2 primeros grupos de V y NV\n",
        "def gen_mini(): #usa yield para hacer pares (x,y) para el subset de mini\n",
        "    # (C) una sola lectura por frame\n",
        "    for fs,y,_ in mini: #cada grupo de estso es una tupla (fs, y, ruta)\n",
        "        sel = muestrear_k(fs, FRAMES_POR_VIDEO) # selecdciona k frames equispaciados de fs y si hay menos repite.\n",
        "        batch = [] #recorre sel, lee cada imagen y preprocesa con leer_redimensionar.cv2.imread de bgr→rgb→img_size→float32→preprocess_input(MobileNetV2: rango [-1,1])\n",
        "        for fp in sel:\n",
        "            arr = leer_redimensionar(fp)\n",
        "            if arr is not None: #si un frame falla, lo omite\n",
        "                batch.append(arr)\n",
        "        if not batch: continue #si ningun frame se puede leer, salta el grupo\n",
        "        while len(batch) < FRAMES_POR_VIDEO: batch.append(batch[-1]) # si faltan frames rellena con el penúltimo hasta tener los 24.\n",
        "        x = np.stack(batch, axis=0) # aplica los k tensores de forma (H,W,3) → un tensor 4D (K, H, W, 3) para un video.\n",
        "        yield x.astype(np.float32), np.int32(y) #saca el par de x con (K,H,W,3) float32 y Y con escalar int32\n",
        "\n",
        "if len(mini) > 0: # solo ejecuta el test si al menos hay un grupo en mini.\n",
        "    ds_mini = tf.data.Dataset.from_generator(gen_mini, output_signature=(spec_x, spec_y)).batch(2) #crea Dataset de TF para genmini, describe forma y tipo, bnatch agrupa en lotes de 2 secuencias.\n",
        "    print(\"\\n[TEST] Overfit rápido a 4 videos…\") #para distinguir en logs (como tipo stream)\n",
        "    model_mini = construir_modelo(base_trainable=True, lr=1e-3) # aqui si uso true, para ver que pasa, ver si memoriza se que para pocos datos es false y para muchos true, solo es una prueba.\n",
        "    hist_mini = model_mini.fit(ds_mini, epochs=4, verbose=0) # 4 epocas solo para ver si sube y aprende.verbose=0 es porque lo quiero logs, solo es chequeo rapido.\n",
        "    print(\"acc mini:\", float(hist_mini.history[\"accuracy\"][-1]), \"auc mini:\", float(hist_mini.history[\"auc\"][-1])) #ver como le fue en accuracy y auc.\n",
        "    #ambos valores suben bien, porque el modelo tiene capacidad de memorizar 2–4 ejemplos, esoe s bueno.\n",
        "\n",
        "# ----------- class_weight simétrico (D) -----------\n",
        "#lo primero es para ver cúantos ejemplos de cada clase hay en el dataset de entrenamiento item_tr es una tupla de creada en split estratificado.\n",
        "n0 = sum(1 for _,y,_ in items_tr if y==0) #número de grupos de clase 0 válidos\n",
        "n1 = sum(1 for _,y,_ in items_tr if y==1) # 1 np válidos.\n",
        "n  = n0 + n1 #número total de grupos de entrenamiento, los sumo.\n",
        "cw = None #inicia (class_weight) como none, por si alguna clase no tuviera datos en ese escenario no se aplican pesos.\n",
        "if n0 > 0 and n1 > 0: #solo calcula pesos si las 2 clases existen.\n",
        "    cw = {0: n/(2*max(1,n0)), 1: n/(2*max(1,n1))} #el factor 2 es porque hay 2 clases el max(1, n0 y n1) son para no dividir sobre 0.\n",
        "    #Así se asegura que cada clase aporte el mismo peso total al entrenamiento, aunque estén desbalanceadas.\n",
        "\n",
        "# ----------- callbacks (E) -----------\n",
        "#\n",
        "cb = [ #creo la lista cb que tiene modelcheckpoint y earlystopping, keras sabra que tiene que usar estos callbacks cuando use model.fit (entrenamiento)\n",
        "  tf.keras.callbacks.ModelCheckpoint( #es la clase que crea el callback y guarda cuando la metrica de validación mejora\n",
        "      MODELO_SALIDA, monitor=\"val_auc\", save_best_only=True, mode=\"max\"), #nombre,CB va a mirar datos de validación (que tan bien separa), solo se guardara cuando mejore el val_auc, mientras más alto auc mejor modelo .\n",
        "  tf.keras.callbacks.EarlyStopping( # crea el CB evita que siga entrenando de más y sobreajuste y asegura los mejores pesos posibles.\n",
        "      monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True) #lo mismo, mas auc mejor, después de 3 épocas si no mejora detienen,\n",
        "] #importante que cuando para no se queda con los pesos de la última época (puede que no sea buena) restaura a pesos del modelo de la época edl mejor AUC.\n",
        "\n",
        "# ----------- entrenamiento real -----------\n",
        "#model fit hace que keras empiece a iterar, calcular pérdidas, actualizar pesos y validar las epocas.\n",
        "#ds_tr es el data set de entrenamiento ya creado, tiene pares (x,y) con x (4,24,160,160,3) y xon Y la etiqueta escalar 0 o 1\n",
        "#La DataSet de validación es para medir el rendimiento de las epochs, ve si el modelo sobreajusta los val_loss, val_accuracy, val_auc salen de aquí.\n",
        "#epochs es rl número de veces que el modelo verá todo el dataset de entrenamiento. (gracias al earlystopping, el entrenamiento puede terminar antes si no mejora el AUC.)\n",
        "#Class_weigth=cw se aplican los pesos que ya calculamos (cw = {0: w0, 1: w1} Durante el cálculo de pérdida (binary_crossentropy), cada muestra se multiplica por su peso.\n",
        "#util porque asi se equilibran datasets desbalanceados (los errores en la clase minoritaria cuentan más)\n",
        "#callbacks=cb son los CB de antes, MC: guarda el mejor modelo según val_auc y ES: detiene entrenamiento si no mejora AUC en 3 epochs seguidos, y restaura el mejor.\n",
        "#El resultado se guarda en history history.history tiene todos los logs de entrenamiento por época\n",
        "#loss: pérdida en entrenamiento por epoch. accuracy: precisión en entrenamiento. auc: AUC en entrenamiento. val_loss, val_accuracy, val_auc: las mismas pero en validación.\n",
        "history = model.fit(ds_tr, validation_data=ds_val, epochs=EPOCHS, class_weight=cw, callbacks=cb)\n",
        "\n",
        "\n",
        "#mensaje en consola para saber que se guardo bien. Nota si se entrenan 10 épocas, pero fue la 7 le mejor,se guarda la epoch 7.)\n",
        "model.save(MODELO_SALIDA)\n",
        "print(f\"[OK] Modelo guardado en: {MODELO_SALIDA}\")\n",
        "\n",
        "# ----------- (Opcional) Fine-tuning breve -----------\n",
        "# Descomenta para afinar últimas capas:\n",
        "# model.get_layer(index=1).trainable = True  # 'base' es el segundo layer en este grafo\n",
        "# for l in model.get_layer(index=1).layers[:-5]:  # congela todo menos ~5 capas finales\n",
        "#     l.trainable = False\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "#               loss=\"binary_crossentropy\",\n",
        "#               metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "# history_ft = model.fit(ds_tr, validation_data=ds_val, epochs=3, class_weight=cw, callbacks=cb)\n",
        "# model.save(MODELO_SALIDA)\n",
        "# print(\"[OK] Modelo fine-tuned guardado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1MZK6xQYTzv"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Celda A — CONFIG + HELPERS\n",
        "# =========================================================\n",
        "import os, re, cv2, random, numpy as np, tensorflow as tf\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "#os es para navegar en carpetas sin os no se podria listar frames desde las carpetas, usos: (os.listdir, os.path.join, os.path.isdir, os.path.isfile.)\n",
        "#re es para que los frames se lean en el orden temporal correcto, se usa en natkey(s)\n",
        "#usos de cv2: cv2.imread: leer imágenes. cv2.cvtColor: cambiar color BGR→RGB. cv2.resize: redimensionar a (160,160), sirve para preparar los frames antes de pasarlos al modelo.\n",
        "#usos de random: random.seed(SEED): fijar semilla → reproducibilidad. random.shuffle: mezclar listas (split_estratificado). (evita que haya resultados diferentes en cada ejecución)\n",
        "#numpy: np.linspace:equiespaciar frames. np.stack:imágenes en tensores. np.float32:datos TensorFlow. np.median, np.diff, np.convolve: cálculos matemáticos.\n",
        "#Es el puente entre OpenCV (que devuelve arrays) y TensorFlow (que espera tensores).}\n",
        "#tf: Construye modelo (keras.layers, keras.models). Definir datasets (tf.data.Dataset.from_generator). Funciones de entrenamiento (model.fit, class_weight, callbacks).\n",
        "#es el motor que ejecuta el entrenamiento de la red neuronal.\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "#Uso una variable RAIZ para evita repetir rutas largas asi se facilita mover el proyecto a otra ubicación si se cambiara la raíz.\n",
        "RAIZ = \"/content/drive/My Drive/videoscombi\"\n",
        "RUTA_VALIDOS   = os.path.join(RAIZ, \"videosvalidos\") #aca el os es para no concatenar strings manualmente (como dato: Windows usa \\, Linux/Mac usan /)\n",
        "RUTA_NOVALIDOS = os.path.join(RAIZ, \"videosnovalidos\")\n",
        "CARPETA_RESULT = os.path.join(RAIZ, \"resultados\") #ruta para guardar la salida del modelo (overlays y csv con resultados de distancia, calificaciones)\n",
        "os.makedirs(CARPETA_RESULT, exist_ok=True) #crea la carpeta resultados si no existe, porque luego el codigo va a g uardar overlays y CSV en esat carpeta.\n",
        "\n",
        "#ruta donde se guarda el modelo entrenado\n",
        "MODELO_SALIDA = \"/content/drive/My Drive/modelo_saltos_novalidos_seq.h5\"\n",
        "RUTA_CSV      = \"/content/drive/My Drive/reporte_saltos.csv\"\n",
        "#contendrá en juez un dataframe: grupo:nombre de la carpeta/video procesado. p_inv:probabilidad de invalidez. invalido:si fue clasificado como inválido o no. distancia_m:distancia en m. calificacion: etiqueta tipo “normalito”, “muy bien”, “perfecto”.\n",
        "\n",
        "IMG_SIZE = (160, 160)        # buen equilibrio entre calidad y costo computacional\n",
        "FRAMES_POR_VIDEO = 24        # 24 frames representativos por salto\n",
        "BATCH_SIZE = 4               #si tienes vram buena subelo,para que este más rapido el entrenamiento\n",
        "VAL_SPLIT  = 0.20            # porcentaje destinado a la validación\n",
        "EPOCHS     = 6               # epocas\n",
        "SEED       = 42              # determinista (puedes poner la que quieras puse ese número por referencia a La guía del autoestopista galáctico)\n",
        "ESCALA_METROS = 13.20        # es el factor de conversión a metros porque MediaPipe devuelve posiciones normalizadas en la imagen (x,y entre 0 y 1)\n",
        "#para la distancia real multiplico por ese factor de escala se fija para que sea consistente con todo el pipeline (dataset, inferencia, juez).\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED) #para que sea 100% reproducible pongo las 3.\n",
        "#El split estratificado siempre separa los mismos videos en train/val. Los frames seleccionados por muestrear_k serán siempre iguales.\n",
        "#El modelo empieza con los mismos pesos iniciales en cada ejecución. Dropout aplica las mismas máscaras en entrenamientos repetidos.\n",
        "#(lo de las mascaras asi se dice cuando se habla de que neuronas se van a apagar)\n",
        "# ---------- ORDEN / IO ----------\n",
        "def natkey(s): #orden natural\n",
        "\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", str(s))] #divide las cedanas en bloques de texto y números.\n",
        "    #(el (\\d+) es para 1 o más dígitos seguidos como esta en paréntesis se devuelven como elementos de la lista)\n",
        "#filtras archivos que son imágenes con p en minúsculas\n",
        "def es_imagen(p):\n",
        "    return str(p).lower().endswith((\".jpg\",\".jpeg\",\".png\")) #revisa la extensión, true si es imagen, false si es otra cosa\n",
        "\n",
        "def listar_subcarpetas(carpeta): #todas las subcarpetas a una carpeta\n",
        "    try:\n",
        "        subs = [os.path.join(carpeta,d) for d in os.listdir(carpeta) # devuelve todos los nombrees de archivos de \"carpeta\"\n",
        "                if os.path.isdir(os.path.join(carpeta,d))] # se queda solo con los que son directorios. y construye ruta completa.\n",
        "    except FileNotFoundError: # si carpeta no existe\n",
        "        return [] #y devuelve una lista vacía.\n",
        "    return sorted(subs, key=natkey) #devuelve la lista ordenada de manera natural.\n",
        "\n",
        "def listar_frames_en_carpeta(carpeta):\n",
        "    #Filtra overlays/resultados y ordena natural.\n",
        "    if not os.path.isdir(carpeta): return [] #revisa si existe carpeta y es repertorio, si no regresa una lista vacía.\n",
        "    fs, ban = [], (\"resultado\", \"_result\", \"overlay\") #en fs es la lista donde se van a guardar ímagenes válidas y ban para imágenes de procesos anteriores.\n",
        "    for f in os.listdir(carpeta): #f es el nombre del archivo donde se va a iterar sobre todos los nombres de las carpetas\n",
        "        low = f.lower() #lo pasa a minúsculas para comprobar.\n",
        "        if not es_imagen(low): continue #usa la función es_imagen(p) para las extensiones .jpg, png, .jpeg y si no es imagen la salta.\n",
        "        if any(b in low for b in ban): continue #verifica si el nombre del archivo tiene esas palabras, si sí salta.\n",
        "        p = os.path.join(carpeta, f) #construye la ruta del archivo\n",
        "        if os.path.isfile(p): fs.append(p) #verifica que sea archivo y no subdirectorio. si lo es, lo agrega a la lista fs.\n",
        "    return sorted(fs, key=natkey) #devuelve la lista de rutas de ímagenes filtradas y ordena los nombres con el natkey, para orden natural.\n",
        "\n",
        "def listar_videos(carpeta):\n",
        "    if not os.path.isdir(carpeta): return [] #ve si carpeta existe y es un directorio y si no, da una lista vacía.\n",
        "    vids = [] #Inicia una lista vacía donde se guardarán las rutas de videos válidos.\n",
        "    for f in os.listdir(carpeta): #recorre todos los archivos dentro de la carpeta.\n",
        "        low = f.lower() #convierte el nombre en minúsculas.\n",
        "        if not low.endswith((\".mp4\",\".avi\",\".mov\")): continue #filtra por extensión de esos archivo lo demás lo salta.\n",
        "        if low.endswith(\"_result.mp4\") or low.endswith(\"_resultado.mp4\"): continue #excluye videos con esas terminaciones para no analizar imagenes con texto encima, overlays y resultados.\n",
        "        p = os.path.join(carpeta, f) #construye la ruta completa del archivo.\n",
        "        if os.path.isfile(p): vids.append(p) #revisa que sea un archivo y no un directorio, si lo es lo añade a vids.\n",
        "    return sorted(vids, key=natkey) #Devuelve la lista de rutas de videos válidos ordenados naturalmente.\n",
        "\n",
        "# ---------- MUESTREO ----------\n",
        "#fs lista de rutas de frames y k los frames que se usan.\n",
        "def muestrear_k(fs, k): #Equiespaciado determinista, siempre serán los mismos resultados en la lista fs\n",
        "    if not fs: return [] #si fs esta vacío regresa una lista vacía\n",
        "    n = len(fs) #calcula cuantos frames hay de verdad en la carpeta con n el número total de ímagenes en fs.\n",
        "    #np.linspace(0, max(0, n-1) genera k números equi-espaciados entre ellos, 0 primer índice de la lista el otro es el último índice válido.\n",
        "    #max asegura que si n=0 (o sea si la lista esta vacía) no de error (aunque ya tenemos el return de arriba), los float pasan a int.\n",
        "    idx = np.linspace(0, max(0, n-1), num=k, dtype=int) #ya quedaria el array de k índices equiespaciados.\n",
        "    return [fs[i] for i in idx] #Construye una nueva lista con los frames que salieron de idx, Siempre devuelve exactamente k.\n",
        "\n",
        "# ---------- PREPROCESO (CONSISTENTE CON TRAIN/INFERENCIA) ----------\n",
        "\n",
        "def leer_redimensionar(fp):\n",
        "    im = cv2.imread(fp) #carga la imagen en formato BGR (el estándar de OpenCV).\n",
        "    if im is None: return None #si por algo no la puede leer, regresa none.\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) #de BGR a RGB porque TF y MobileNetV2 esperan RGB\n",
        "    im = cv2.resize(im, IMG_SIZE, interpolation=cv2.INTER_AREA) #redimensiona (160,160) e inter para reducción de tamaño, porque promedia píxeles y preserva más información.\n",
        "    im = im.astype(np.float32) pasa de un uint8(sin signo, entero, 8bits de 0n a 255b y pues es en grises) a un float 32, eso espera TF.\n",
        "    return preprocess_input(im)  # pasa de [0,255] a [-1,1] #es una función oficial de Keras para MobileNetV2.\n",
        "\n",
        "# ---------- AGRUPACIÓN ----------\n",
        "#organizar losdatos en un formato uniforme, muy útil para entrenar.\n",
        "def armar_grupos(ruta, etiqueta): #donde estan los frames (puede ser subcarpetas o incluso sueltos) y 0 si válido 1 si inválido.\n",
        "#Devuelve lista de tuplas: (lista_frames, etiqueta, ruta_grupo).\n",
        "    grupos = []\n",
        "    for sub in listar_subcarpetas(ruta): #busca las subcarpetas dentro de ruta.\n",
        "        fs = listar_frames_en_carpeta(sub) #obtiene la lista de frames.\n",
        "        if fs: grupos.append((fs, etiqueta, sub)) #si la subcarpeta tiene frames (if fs) agrega un grupo con (fs(listaframes encontrados), etiqueta, ruta)\n",
        "    fs_root = listar_frames_en_carpeta(ruta)  #revisa si en la raíz (ruta) hay frames sueltos, si los hay los mete como grupo\n",
        "    if fs_root: grupos.append((fs_root, etiqueta, ruta)) #el grupo es de esa forma.\n",
        "    return grupos #devuelve todos los grupos encontrados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "ExQWMIuQguAC",
        "outputId": "9361bf70-9b8c-468d-9bb3-d20870d42941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] grupos válidos: 83 | no válidos: 19\n",
            "Train V/NV: 67 16\n",
            "Val   V/NV: 16 3\n",
            "[INFO] train videos: 83 | val videos: 19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_8              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_9              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_8              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_9              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TEST] Overfit rápido a 4 videos…\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# Celda B — ENTRENAMIENTO\n",
        "# =========================================================\n",
        "#tf = base. layers = bloques para construir. models = ensamblador final.\n",
        "#para sefinir el modelo construir_modelo con models.Model(...).\n",
        "#Insertar capas con layers.TimeDistributed, layers.GlobalAveragePooling2D, layers.Dense, para eso ocupo estos imports.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- Recolectar grupos ---\n",
        "#aqui se arma el dataset inicial a partir de los frames que ya tengo en las carpetas.\n",
        "gr_validos   = armar_grupos(RUTA_VALIDOS,   0) #Busca subcarpetas o frames sueltos en RUTA_VALIDOS y RUTA_NOVALIDOS.\n",
        "gr_novalidos = armar_grupos(RUTA_NOVALIDOS, 1) ##Devuelve una lista de tuplas (lista_de_frames, y (0 en válido o 1), ruta_del_grupo)\n",
        "print(f\"[INFO] grupos válidos: {len(gr_validos)} | no válidos: {len(gr_novalidos)}\") # muestra cuantos grupos de salto hay de cada clase (número de videos representados en tu dataset).\n",
        "if (len(gr_validos)+len(gr_novalidos)) == 0: #len para saber cuantos elementos tiene la lista, si esa suma es 0 no hay nada.\n",
        "    raise RuntimeError(\"No hay frames en las carpetas. Verifica rutas y extensiones.\") #para eso sirve raise detiene la ejecución si suma 0.\n",
        "\n",
        "# --- Split estratificado ---\n",
        "#separa los train y los de validación cuidando que válidos y no válidos esten representados en cada conjunto (para no tener todos los válidos en train y todos los inválidos en val, por si acaso)\n",
        "def split_estratificado(V, NV, val_split=VAL_SPLIT, seed=SEED): #(válidos, no válidos, % de validación, para que el barajeo sea reproducible.)\n",
        "    random.seed(seed)\n",
        "    V = V.copy(); NV = NV.copy()\n",
        "    random.shuffle(V); random.shuffle(NV) #Mezclas los elementos de cada lista independientemente, para que no queden igual.\n",
        "    nV_val  = max(1, int(len(V)*val_split)) if len(V)>0 else 0 #cuantos NV y V van a validación. int es un entero y redondea abajo.\n",
        "    nNV_val = max(1, int(len(NV)*val_split)) if len(NV)>0 else 0 #el 0 se asegura de que al menos 1 grupo vaya a validación.\n",
        "    items_val = V[:nV_val] + NV[:nNV_val] #se toman los primeros V y NV para Validación.\n",
        "    items_tr  = V[nV_val:] + NV[nNV_val:] #lo que sobra se queda en entrebamiento (train)\n",
        "    random.shuffle(items_val); random.shuffle(items_tr) #mezcla los 2 conjuntos para que no queden primero válidos y luego no válidos.\n",
        "    return items_tr, items_val #devuelve 2 listas de items con forma (frames, y, ruta) items_tr: dataset de entrenamiento y items_val:dataset de validación.\n",
        "\n",
        "\n",
        "#aca ya se usa el split, le doy todos los grupos válidos recolectados antes con armar_grupos y los no válidos.\n",
        "#me da items_tr que son los grupos seleccionados para entrenamiento y items_val los de validación.\n",
        "# for _,y,_ in items_tr porque solo me interesa Y, o sea si es válido 0 o no 1, if y==0 solo cuenta los grupos válidos. y y==1 no válidos.\n",
        "#el 1 for... if... Si y==0 devuelve 1 si no, no devuelve nada. (produce una secuencia de 1s tantas veces como haya grupos válidos.)\n",
        "#el sum, suma todos esos números, cuantos cumplen esa condición y el print tiene  forma de Train V/NV: 2 1 (en el set de entrenamiento hay 2 válidos y 1 no válido.)\n",
        "items_tr, items_val = split_estratificado(gr_validos, gr_novalidos)\n",
        "print(\"Train V/NV:\", sum(1 for _,y,_ in items_tr if y==0), sum(1 for _,y,_ in items_tr if y==1))\n",
        "print(\"Val   V/NV:\", sum(1 for _,y,_ in items_val if y==0), sum(1 for _,y,_ in items_val if y==1))\n",
        "\n",
        "# --- Generador + tf.data ---\n",
        "def gen(items_lista, k=FRAMES_POR_VIDEO): #define un generador que reproducira pares (x,y) para alimentar a tf.data.Dataset. (fs,y,ruta, k=24)\n",
        "    for fs, y, _ in items_lista: #itera cada grupo en una lista de (fs, y, sin la ruta por eso el _ ) lista de frames del grupo y etiqueta (0 válido 1 no válido)\n",
        "        sel = muestrear_k(fs, k) #agarra k frames de fs (equiespaciado y determinista) si hay menos, muestrear repite bordes hasta k. (para longitud fija)\n",
        "        batch = [] # para los frames de este grupo\n",
        "        for fp in sel: #para cada ruta fp seleccionada.\n",
        "            arr = leer_redimensionar(fp) #Lee la imagen. BGR→RGB. Resize a IMG_SIZE (p.ej. 160×160). float32. preprocess_input de MobileNetV2 → rango [-1, 1].\n",
        "            if arr is not None: batch.append(arr) #si algo falla, none, lo ignora ese frame.\n",
        "        if not batch:  # si ningun frame de ese grupo se pudo leer, lo salta.\n",
        "            continue\n",
        "        while len(batch) < k:  #si despues de filtrar hay menos de k frames rellena hasta llegar a k.\n",
        "            batch.append(batch[-1])\n",
        "        x = np.stack(batch, axis=0)  #mete a k arrays (H,W,3) en un solo tensor (K, H, W, 3)\n",
        "        yield x.astype(np.float32), np.int32(y) # aca un par (x,y) X en float32 y forma (K,H,W,3) y con Y en int32 y es escalar (0 o 1).\n",
        "\n",
        "#aquí no aparece el batch porque eso lo agrega tf.data.Dataset.batch(). O sea, antes(24,160,160,3) Después(BATCH_SIZE,24,160,160,3)\n",
        "#spec_x es para describir la entrada de mi modelo (salto en k frames) la shape es de (24, 160x160, RGB) en float32. x= (24,160,160,3)\n",
        "#spec_y es la etiqueta,, es escalar() y tiene forma int32 un entero (0 = válido, 1 = no válido).\n",
        "spec_x = tf.TensorSpec(shape=(FRAMES_POR_VIDEO, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32)\n",
        "spec_y = tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "\n",
        "#se crea un Dataset a partir de mi función gen(items_tr). gen hace pares (x,y) con x un tensor (FRAMES_POR_VIDEO, H, W, 3) que son los frames procesados de un salto.\n",
        "# Y un escalar 0 o 1 indicando válido/no válido, el out es lo que le dice a TF que va a salir.\n",
        "#hace shuffle en un buffer de tamaño 256 antes de entregarlos. (para que la red no vea primero V y luego NV) seed para que el shuffle sea reproducible.\n",
        "#el batch(BATCH_SIZE) agrupa ejemplos ya no es un (24,160,160,3) ya seria un (BATCH_SIZE,24,160,160,3) tambien en y (BATCH_SIZE,)\n",
        "#el .prefetch(tf.data.AUTOTUNE) Mientras el modelo está entrenando en el lote n, TensorFlow ya está preparando el lote n+1 en paralelo.\n",
        "#eso mejora mucho la velocidad de entrenamiento porque el autotune hace que TF vea número óptimo de lotes para adelantar.\n",
        "#para ds_val es lo mismo pero sin el shuffle para que sea determinista y estable.\n",
        "ds_tr  = (tf.data.Dataset.from_generator(lambda: gen(items_tr),  output_signature=(spec_x, spec_y))\n",
        "          .shuffle(256, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
        "ds_val = (tf.data.Dataset.from_generator(lambda: gen(items_val), output_signature=(spec_x, spec_y))\n",
        "          .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "#cuenta cuántos grupos de entrenamiento (items_tr) hay, cada elemento es una tupla (fs,y,ruta) son los que se usan para entrenar, val es lo mismo pero para validación.\n",
        "print(f\"[INFO] train videos: {len(items_tr)} | val videos: {len(items_val)}\")\n",
        "\n",
        "# --- Modelo ---\n",
        "#defino construir_modelo que regresa una red lista para entrenar.\n",
        "def construir_modelo(img_size=IMG_SIZE, k=FRAMES_POR_VIDEO, base_trainable=False, lr=1e-3): #(160x160, 24, false para congelar (más rapido y avita SA con pocos datos), la LR)\n",
        "    inp = layers.Input(shape=(k, img_size[0], img_size[1], 3)) #es la entrada del modelo (24, 160, 160, 3) le voy a la red un video representado como k imágenes RGB.\n",
        "    #Carga MobileNetV2 sin la “cabeza” (sinx maxpooling ni dropout ni sigmoid), uso pesos preentrenados de imagenet, usamos puro cuerpo aca.\n",
        "    base = tf.keras.applications.MobileNetV2(include_top=False, input_shape=img_size+(3,), weights=\"imagenet\")\n",
        "    base.trainable = base_trainable # controla si se copngela la base o se entrena(false rapido, solo capas superiores y true ya fine tuning)\n",
        "    x = layers.TimeDistributed(base)(inp) #MobileNetV2 a cada frame por separado y sale un mapa de características 2D(5,5,1280) y con los k frames (k, 5, 5, 1280).\n",
        "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x) #aplica GAP frame-frame convierte (5,5,1280) en un vector (1280,). Ahora x = (k, 1280)  que es un vector de 1280 características por frame.\n",
        "    x = layers.GlobalAveragePooling1D()(x)#promedia en el tiempo, junta la info de todos los frames (k ejes de tiempo) y Calcula el promedio de los 24 vectores(1280,).\n",
        "    x = layers.Dropout(0.3)(x)#para evitar overfitting se apaga aleatoriamente neuronas en cada batch.\n",
        "    out = layers.Dense(1, activation=\"sigmoid\")(x) #lo mismo, calcula la probabilidad de que no sea válido.\n",
        "    model = models.Model(inp, out) #Define el modelo con su input (inp) y output (out).\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), #adam como optimizador con su lr\n",
        "                  loss=\"binary_crossentropy\", #esta es la función de perdida y la ocupo porque nuestro problema es binario.\n",
        "                  metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]) #accuracy % de predicciones correctas. AUC área bajo curva ROC mide qué tan bien separa válidos de no válidos.\n",
        "    return model #Devuelve el modelo listo para entrenar o evaluar.\n",
        "\n",
        "model = construir_modelo(base_trainable=False, lr=1e-3) #congelo para entrenar solo capas finales, y el lr es para adam. para tener un tf.keras.Model\n",
        "model.summary() #resumen tabular.\n",
        "\n",
        "# --- (Opcional) sanity check: overfit mini ---\n",
        "#prueba que el pipeline funciona, toma los 2 primeros de válidos y de no válidos.\n",
        "mini = gr_validos[:2] + gr_novalidos[:2]\n",
        "if mini: #lista de 4 tuplas. y solo se ejecuta si no esta vacío.\n",
        "    def gen_mini(): #generador de pares (x,y)\n",
        "        for fs,y,_ in mini: #itera cada grupo (fs,y,_) de mini\n",
        "            sel = muestrear_k(fs, FRAMES_POR_VIDEO) #agarra k frames equespaciados y lo garantiza.\n",
        "            batch = []\n",
        "            for fp in sel:\n",
        "#Lee imagen (OpenCV), BGR a RGB, resize a IMG_SIZE(160×160), float32, preprocess_input de MobileNetV2 ([-1,1]).\n",
        "#Si alguna lectura falla (None), se omite ese frame.\n",
        "                arr = leer_redimensionar(fp)\n",
        "                if arr is not None: batch.append(arr)\n",
        "            if not batch: continue #si ningún frame quedó válido omite ese grupo.\n",
        "            while len(batch) < FRAMES_POR_VIDEO: batch.append(batch[-1]) #repite el último frame hasta acompletar k.\n",
        "            x = np.stack(batch, axis=0) #pasa de(H,W,3) a un tensor (K, H, W, 3) este en nuestro caso((24,160,160,3)).\n",
        "            yield x.astype(np.float32), np.int32(y) #saca pares (x,y) con x:(K,H,W,3) float32 (preprocesado) Y: escalar int32 (0/1).\n",
        "#Crea un Dataset de TF desde el generador, el out define shapes y dtypes en spec_x: (K,H,W,3) float32. spec_y: () int32 y el batch agrupa de 2 en 2\n",
        "    ds_mini = tf.data.Dataset.from_generator(gen_mini, output_signature=(spec_x, spec_y)).batch(2) #x:(2, K, H, W, 3) ((2,24,160,160,3)). Y:(2,).\n",
        "    print(\"\\n[TEST] Overfit rápido a 4 videos…\") #mensaje\n",
        "    #modelop para el test aca descongelo para que el modelo puede memorizar fácilmente 2/4 ejemplos (sanity check).\n",
        "    model_mini = construir_modelo(base_trainable=True, lr=1e-3) #el LR esta legal para estos procedimientos.\n",
        "    hist_mini = model_mini.fit(ds_mini, epochs=4, verbose=0) #entrena 4 epocas en donde todo debe subir (loss ↓ accuracy ↑ auc ↑)\n",
        "    print(\"acc mini:\", float(hist_mini.history[\"accuracy\"][-1]), \"auc mini:\", float(hist_mini.history[\"auc\"][-1]))#de este test\n",
        "    #si salio bien, el modelo tiene capacidad para memorizar 4 ejemplos (deberia andar por .9 o más)\n",
        "\n",
        "# --- class_weight (dar un poco más de peso a 'no válido') ---\n",
        "#los no válidos que son los menos valen más en la función de pérdida, forzando al modelo a prestarles atención.\n",
        "#Clase 0 (válidos): peso fijo 1.0. Clase 1 (no válidos): se le da un peso extra:\n",
        "n0 = sum(1 for _,y,_ in items_tr if y==0) #items es tu lista de grupos del split de entrenamiento (fs,y,ruta)\n",
        "n1 = sum(1 for _,y,_ in items_tr if y==1) #cuántos grupos hay de cada clase 0 válido y 1 no válido.\n",
        "cw = None #empiezo cw en none porque por si hay alguna clase vacía, para no aplicar pesos\n",
        "#cw es un diccionario con pesos para pasar a model.fit(class_weight=cw)\n",
        "if n0 > 0 and n1 > 0: #solo si hay 1 V y 1 NV se calcula el ratio.\n",
        "    ratio = n0 / max(1, n1) # cuántos válidos hay por cada no válido.\n",
        "    cw = {0: 1.0, 1: min(2.0, 1.2*ratio)}  # 1.2–2.0 según desbalance\n",
        "#1.2 * ratio (para compensar menos no válidos). Se limita con min(2.0, …)nunca se pasa de 2.0.\n",
        "\n",
        "# --- callbacks ---\n",
        "#ModelCheckpoint: guarda el modelo durante el entrenamiento.\n",
        "#monitor=\"val_auc\": se fija en la métrica AUC de validación.\n",
        "#save_best_only=True: solo guarda el modelo cuando mejora el valor monitorizado (no en cada epoch).\n",
        "#mode=\"max\": como AUC es mejor cuanto más alto, guarda el modelo cuando aumenta la métrica.\n",
        "cb = [\n",
        "  tf.keras.callbacks.ModelCheckpoint(MODELO_SALIDA, monitor=\"val_auc\", save_best_only=True, mode=\"max\"),\n",
        "  tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True)\n",
        "]\n",
        "#para early corta el entrenamiento antes si ya no mejora.\n",
        "#monitor=\"val_auc\": observa la misma métrica AUC de validación.\n",
        "#mode=\"max\": espera a que mejore\n",
        "#patience=3: si pasan 3 epochs seguidas sin mejora, se detiene.\n",
        "#restore_best_weights=True: al parar, restaura los pesos de la mejor epoch (no los últimos). para ahorrar tiempo y sobreajuste\n",
        "\n",
        "# --- Entrenar ---\n",
        "#model fit llama a keras aca el modelo recibe datos, ajusta pesos, calcula errores, mejora y guarda.\n",
        "#ds_tr es el dataset de entrenamiento (tf.data.Dataset)(generadores). Contiene las secuencias de frames con válidos (0) o no válidos (1).\n",
        "#validation...Es el dataset de validación, para medir qué tan bien se generaliza. para detectar sobreajuste+\n",
        "#si la precisión de entrenamiento sube pero la validación se queda igual o baja, significa que el modelo memoriza pero no generaliza.\n",
        "#epochs es el número máximo de pasadas completas por todo el dataset con el cb ES,acaba si no mejora (class_weight=cw)\n",
        "#le da peso extra a los NV y los cb MC:guarda el mejor modelo según val_auc.ES: corta el entrenamiento si no hay mejoras.\n",
        "#history guarda los valores de las métricas en cada epoch. (loss, accuracy,auc etc)\n",
        "history = model.fit(ds_tr, validation_data=ds_val, epochs=EPOCHS, class_weight=cw, callbacks=cb)\n",
        "\n",
        "#se ejecuta el pipeline: datasets, pesos de clases, validación y callbacks. al final history es para ver cómo evolucionó el modelo.\n",
        "\n",
        "# guarda la arquitectura del modelo + pesos + configuración de entrenamiento un .h5\n",
        "model.save(MODELO_SALIDA)\n",
        "print(f\"[OK] Modelo guardado en: {MODELO_SALIDA}\")\n",
        "\n",
        "# --- (Opcional) Fine-tuning corto ---\n",
        "# base = model.layers[1].layer  # TimeDistributed(base)\n",
        "# for l in base.layers[:-6]: l.trainable = False\n",
        "# for l in base.layers[-6:]:  l.trainable = True\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "#               loss=\"binary_crossentropy\",\n",
        "#               metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "# model.fit(ds_tr, validation_data=ds_val, epochs=3, class_weight=cw, callbacks=cb)\n",
        "# model.save(MODELO_SALIDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2BQ02W1gzl7",
        "outputId": "8bc04acf-d503-4349-9618-102724ad254c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] grupos totales: 102 (V=83, NV=19)\n",
            "vn1: Invalido (p_inv=0.70)\n",
            "vn2: 6.56 m • muy bien (p_inv=0.28)\n",
            "vn3: 8.24 m • Muy Bueno (p_inv=0.44)\n",
            "vn4: 10.52 m • perfecto (p_inv=0.39)\n",
            "vn5: 7.47 m • muy bien (p_inv=0.16)\n",
            "vn6: Invalido (p_inv=0.81)\n",
            "vn7: 6.87 m • muy bien (p_inv=0.35)\n",
            "vn8: Invalido (p_inv=0.82)\n",
            "vn9: Invalido (p_inv=0.79)\n",
            "vn10: 5.65 m • normalito (p_inv=0.47)\n",
            "vn11: Invalido (p_inv=0.90)\n",
            "vn12: 5.90 m • normalito (p_inv=0.48)\n",
            "vn13: Invalido (p_inv=0.74)\n",
            "vn14: Invalido (p_inv=0.78)\n",
            "vn15: Invalido (p_inv=0.86)\n",
            "vn16: Invalido (p_inv=0.85)\n",
            "vn17: Invalido (p_inv=0.83)\n",
            "vn18: Revisar (p_inv=0.55)\n",
            "vn19: Invalido (p_inv=0.81)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-48-46: 9.54 m • perfecto (p_inv=0.43)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-48-46_result: 9.57 m • perfecto (p_inv=0.39)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-49-07: 9.83 m • perfecto (p_inv=0.33)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-49-07_result: 9.75 m • perfecto (p_inv=0.27)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-50-07: 7.42 m • muy bien (p_inv=0.32)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-50-07_result: 7.43 m • muy bien (p_inv=0.30)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-51-58: 8.17 m • Muy Bueno (p_inv=0.26)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-51-58_result: 7.42 m • muy bien (p_inv=0.23)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-52-29: 6.59 m • muy bien (p_inv=0.47)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-52-29_result: 6.44 m • muy bien (p_inv=0.43)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-52-53: Invalido (p_inv=0.65)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-52-53_result: Revisar (p_inv=0.54)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-53-33: Revisar (p_inv=0.59)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-53-33_result: Revisar (p_inv=0.53)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-55-16: Revisar (p_inv=0.56)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-55-16_result: 8.84 m • perfecto (p_inv=0.49)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-55-42: 8.02 m • Muy Bueno (p_inv=0.44)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-55-42_result: 8.02 m • Muy Bueno (p_inv=0.39)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-56-39: 6.83 m • muy bien (p_inv=0.18)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-56-39_result: 6.82 m • muy bien (p_inv=0.17)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-57-03: 6.39 m • muy bien (p_inv=0.34)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-57-03_result: 6.38 m • muy bien (p_inv=0.30)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-48-56: 0.36 m • normalito (p_inv=0.24)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-48-56_result: 7.67 m • Muy Bueno (p_inv=0.23)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-53-48: 7.26 m • muy bien (p_inv=0.10)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-53-48_result: 7.26 m • muy bien (p_inv=0.09)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-54-05: 7.16 m • muy bien (p_inv=0.10)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-54-05_result: 7.18 m • muy bien (p_inv=0.10)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-54-34: 5.90 m • normalito (p_inv=0.35)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-54-34_result: 5.92 m • normalito (p_inv=0.33)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-55-54: 9.19 m • perfecto (p_inv=0.28)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-55-54_result: 8.84 m • perfecto (p_inv=0.26)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-56-20: 6.13 m • muy bien (p_inv=0.32)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-56-20_result: 6.20 m • muy bien (p_inv=0.27)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-57-22: 6.85 m • muy bien (p_inv=0.19)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-57-22_result: 6.89 m • muy bien (p_inv=0.18)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-57-47: 9.32 m • perfecto (p_inv=0.22)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-57-47_result: 9.34 m • perfecto (p_inv=0.21)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-58-15: 6.19 m • muy bien (p_inv=0.47)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-58-15_result: 6.13 m • muy bien (p_inv=0.43)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-59-42: 6.07 m • muy bien (p_inv=0.42)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 16-59-42_result: 6.05 m • muy bien (p_inv=0.35)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-00-18: 6.61 m • muy bien (p_inv=0.10)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-00-18_result: 6.63 m • muy bien (p_inv=0.10)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-01-02: 8.30 m • Muy Bueno (p_inv=0.10)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-01-02_result: 8.59 m • perfecto (p_inv=0.08)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-01-54: 8.87 m • perfecto (p_inv=0.14)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-01-54_result: 8.88 m • perfecto (p_inv=0.12)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-08-06: Invalido (p_inv=0.71)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-08-06_result: Invalido (p_inv=0.66)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-09-40: 8.66 m • perfecto (p_inv=0.16)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-09-40_result: 6.73 m • muy bien (p_inv=0.13)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-10-35: 6.55 m • muy bien (p_inv=0.12)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-10-35_result: 6.45 m • muy bien (p_inv=0.11)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-11-27: 7.02 m • muy bien (p_inv=0.17)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-11-27_result: 6.84 m • muy bien (p_inv=0.14)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-13-43: 7.76 m • Muy Bueno (p_inv=0.47)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-13-43_result: 7.87 m • Muy Bueno (p_inv=0.43)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-16-00: 7.05 m • muy bien (p_inv=0.23)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-16-00_result: 7.13 m • muy bien (p_inv=0.19)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-16-32: Invalido (p_inv=0.61)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-16-32_result: Revisar (p_inv=0.57)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-17-03: 6.19 m • muy bien (p_inv=0.12)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-17-03_result: 6.19 m • muy bien (p_inv=0.11)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-17-44: Revisar (p_inv=0.60)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-17-44_result: Revisar (p_inv=0.53)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-18-13: 7.42 m • muy bien (p_inv=0.26)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-18-13_result: 7.42 m • muy bien (p_inv=0.23)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-26-27: 7.50 m • muy bien (p_inv=0.05)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-26-27_result: 9.27 m • perfecto (p_inv=0.05)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-28-34: 8.19 m • Muy Bueno (p_inv=0.30)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-28-34_result: 8.17 m • Muy Bueno (p_inv=0.26)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-29-56: 7.38 m • muy bien (p_inv=0.07)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-29-56_result: 7.29 m • muy bien (p_inv=0.05)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-30-44: Invalido (p_inv=0.65)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-30-44_result: Revisar (p_inv=0.60)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-32-07: 9.23 m • perfecto (p_inv=0.14)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-32-07_result: 9.23 m • perfecto (p_inv=0.13)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-35-34: 5.73 m • normalito (p_inv=0.47)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-35-34_result: 6.64 m • muy bien (p_inv=0.38)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-37-32: 7.16 m • muy bien (p_inv=0.40)\n",
            "Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-37-32_result: 7.17 m • muy bien (p_inv=0.31)\n",
            "Tracker 2021-10-05 17-51-07: Sin coordenadas (p_inv=0.19)\n",
            "\n",
            "[OK] CSV: /content/drive/My Drive/reporte_saltos.csv\n",
            "[TIME] Total: 1744.2s  | BATCH=16 | OVERLAYS=off\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# Celda C — INFERENCIA + JUEZ\n",
        "# =========================================================\n",
        "#os para que el “juez” recorra grupos, para localizar videos/frames, y escribir salidas (CSV/overlays).\n",
        "#cv 2 (opencv) para lectura y escritura de imágenes y videos (cv2.imread, VideoCapture, VideoWriter).\n",
        "#convierte BGR a RGB, resize, dibuja textos y rectángulos para overlays. (cargar frames,poner cartel Inválido/distancia,exportar video_resultado.mp4.)\n",
        "#sys detectar entorno o pasarle a subprocess el intérprete actual (sys.executable).\n",
        "#subprocess en mi pipeline lo uso para instalar dependencias on-the-fly(se descarga instala y corre sobre la marcha, mediapipe) por si no estan.\n",
        "#time para medir tiempos (time.time()), pausas (time.sleep()), marcar logs, cuánto tarda procesar cada grupo/video.\n",
        "#estos son un estandar de sistema y utilidades los siguientes son númericos, de ciencia de datos y deep learning.\n",
        "import os, cv2, sys, subprocess, time\n",
        "#numpy para np.stack, np.median, np.linspace, np.diff, np.convolve lo ocupo para empaquetar secuencias, suavizar señales (x,y) de MediaPipe y cálculos de métricas/umbrales.\n",
        "#pandas para Tablas (DataFrames) reporte final juez, junto resultados: probabilidad p_inv, flag invalido, distancia_m, calificacion y guardas a CSV.\n",
        "#tf carga el modelo entrenado (tf.keras.models.load_model), hacer predict, define CB. También para tipos (tf.float32) y utilidades.\n",
        "import numpy as np, pandas as pd, tensorflow as tf\n",
        "#para cachear resultados y hacer memoización (si usas mucho una funcion no se calcula, se guarda y se ocupa)\n",
        "#Lectura + preprocesado varias veces. Listados de carpetas si consultas repetidamente,evita recomputación,acelera juez cuando hay reutilización.\n",
        "from functools import lru_cache\n",
        "#from tensorflow.keras.applications.mobilenet_v2 import preprocess_input Normalización oficial de MobileNetV2:\n",
        "#convierte píxeles a float32 en rango [-1, 1].para que la inferencia sea coherente con el entrenamiento (misma escala de entrada).\n",
        "#se aplico en leer_redimensionar(fp) antes de pasar al modelo.\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# ---------- PARÁMETROS ----------\n",
        "UMBRAL_INVALIDO  = 0.57     # baja un poco para subir recall (más no válido, si da muchos NV, subir)\n",
        "MARGEN_OK        = 0.04     # banda dudosa estrecha (antes 0.10 mientras más alto más dudoso)\n",
        "N_BATCH          = 16       # grupos por batch en GPU/TPU\n",
        "GENERAR_OVERLAYS = False    #activar al cierre o para compartir resultados visuales; mantenlo en False en desarrollo para iterar rápido. (T para render)\n",
        "K                = FRAMES_POR_VIDEO #los 24 equiespaciados que representan el salto\n",
        "SIZE             = IMG_SIZE #160x160\n",
        "\n",
        "# ---------- CARGA MODELO ----------\n",
        "#carga un modelo previamente entrenado desde disco. (.h5 acá)\n",
        "clf_seq = tf.keras.models.load_model(MODELO_SALIDA) #ruta donde se guardo en la fse de entrenamiento(MODELO_SALIDA = \"/content/drive/My Drive/modelo_saltos_novalidos_seq.h5\")\n",
        "#clf_seq = classifier sequence (clasificador de secuencias) (se usará en p = clf_seq.predict(x))\n",
        "#La arquitectura del modelo (capas y conexiones).\n",
        "#Los pesos entrenados (valores numéricos aprendidos).\n",
        "#La configuración de compilación (optimizador, pérdida, métricas) para generalizar.\n",
        "\n",
        "# ---------- PREPROCESAMIENTO (igual que en entrenamiento) ----------\n",
        "#si se usa el mismo frame varias veces (o en distintos muestreos), ya no lo vuelve a leer ni procesar(acelera mucho la inferencia.)\n",
        "@lru_cache(maxsize=300)#los resultados de esta función se guardan en memoria caché.(se recordarán hasta 300 llamadas distintas en fp)\n",
        "#aca empiezo con _ el nombre para recordarme a mi mismo que (o a alguien más) es de uso interno.\n",
        "#(no deberia usarla directamente desde fuera porque podría cambiar o eliminarse sin previo aviso es un detalle interno es como si fuera privada)\n",
        "def _load_preproc(fp): #recibe fp (file path) o seala ruta del archivo de imagen que se quiere procesar.\n",
        "    im = cv2.imread(fp) #lee desde el disco en formato bgr.\n",
        "    if im is None: return None #si falla regresa none.\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) #paso de bgr que usan open cv a rgb que usa TF\n",
        "    im = cv2.resize(im, SIZE, interpolation=cv2.INTER_AREA) # 160x160 como en entrenamiento I_A reducciones de tamaño es más preciso y suave.\n",
        "    im = im.astype(np.float32) #Convierte los valores de píxeles de uint8 (0–255) a float32 por keras y MobileNet necesitan float.\n",
        "    return preprocess_input(im)  # normaliza los píxeles[-1,1] para que inferencia se igual en distribución a lo que se uso en entrenamiento.\n",
        "#resumen: Lee la imagen la pasa a RGB.\n",
        "#La escala a (160,160).\n",
        "#La convierte a float32.\n",
        "#La normaliza a [-1,1].\n",
        "#Devuelve un tensor listo para pasar al modelo y por lru_cache si lo ocupo de nuevo esta a la mano, (Devuelve un numpy array con forma (H,W,3).)\n",
        "\n",
        "\n",
        "#otro helper interno en el pipeline de inferencia, sel es lista de rutas de imágenes ya seleccionadas (24 frames equiespaciados de un salto).\n",
        "def _stack_from_sel(sel):\n",
        "    batch = [] #vacía pero se irá llena con los frames preprocesados\n",
        "    for fp in sel: #recorre cada filepath en sel\n",
        "        arr = _load_preproc(fp)\n",
        "        if arr is not None: batch.append(arr) # Si arr no es None (imagen válida), la mete en el batch.\n",
        "    if not batch: return None # si por alguna razon ninguna imagen es válida da un none, para no romper nada más adelante.\n",
        "    while len(batch) < K: batch.append(batch[-1]) #se asegura que hay k frames, rellenando con el último válido.\n",
        "    return np.stack(batch, axis=0).astype(np.float32)#np...0 mete la lista de arays a un solo tensor (24, 160, 160, 3), se mete al modelo este tensor.\n",
        "\n",
        "def _sel_k(fs): #la entrada es fs (lista de rutas de frames)\n",
        "    n = len(fs) #n calcula cuantos frames hay en la lista si la carpeta tiene 200 ímagenes pss n=200\n",
        "    idx = np.linspace(0, max(0, n-1), num=K, dtype=int) #k Nu.Eq entre 0 y n-1. redondea a enteros y cuida si n=0 seria 0 el tope y no -1.\n",
        "    return [fs[i] for i in idx] #construye una nueva lista con las rutas a esos frames seleccionados.\n",
        "\n",
        "# ---------- CLASIFICACIÓN EN BATCH ----------\n",
        "#una función para clasificar varios grupos de frames (un grupo = un salto). de entrada tiene a grupos.\n",
        "#lista de tuplas tipo (nombre, carpeta) que vienen en el recolector (armar_grupos).\n",
        "def clasificar_grupos(grupos):\n",
        "    preds = {} #para guardar las predicciones finales, con clave = carpeta y valor = probabilidad p_inv.\n",
        "    X, meta = [], [] #X es la lista acumuladora de tensores de cada grupo, meta: guardar metadatos (nombre, carpeta) en el mismo orden que X.\n",
        "    for (nom, car) in grupos: #nom=nombre del grupo (\"nv1\") car=ruta de la carpeta donde están los frames de ese grupo.\n",
        "        fs = listar_frames_en_carpeta(car) #fs = lista de frames en la carpeta.\n",
        "        if not fs: #si esta vacia no se puede procesar.\n",
        "            preds[car] = None #se guarda None en preds[car] y pasas al siguiente grupo.\n",
        "            continue\n",
        "        sel = _sel_k(fs) #selecciona k frames equiespaciados.\n",
        "        x = _stack_from_sel(sel)#aplica esos frames a un tensor (K,H,W,3). (24,160,160,3)\n",
        "        if x is None:\n",
        "            preds[car] = None #si algo falla, regresa none en predicciones y salta.\n",
        "            continue\n",
        "        X.append(x) #se guarda el tensor del grupo (x) en la lista X, guardo los metadatos (nombre, carpeta) en meta.\n",
        "        meta.append((nom, car)) #Ahora ambos están sincronizados: X[i] corresponde a meta[i].\n",
        "        if len(X) == N_BATCH: #cuando se acumulan el número de batchs necesarios se procesa.\n",
        "            xb = np.stack(X, axis=0)  # junta la lista en un tensor 5D de forma(B,K,H,W,3) b es el tamaño del batch\n",
        "            pb = clf_seq(xb, training=False).numpy().reshape(-1) #ejecuta el modelo en modo inferencia(da probas)convierte a numpy y aplana a vector 1D.\n",
        "            for (n,c), p in zip(meta, pb): preds[c] = float(p) #Asocia cada carpeta c con su probabilidad p.\n",
        "            X, meta = [], [] #Limpias X y meta para acumular el siguiente lote.\n",
        "\n",
        "#Tras salir del bucle, puede que queden grupos pendientes menores a N_BATCH.\n",
        "#Este bloque asegura que esos también se procesen.\n",
        "#X es una lista de tensores (K,H,W,3), uno por grupo. np.stack...0 los junta por eso ya ES 5D.\n",
        "#clf_seq espera entrada (B,K,H,W,3). Procesa cada grupo de frames y produce una probabilidad por grupo (salida de la capa Dense(1, sigmoid)) (B,1) como salida.\n",
        "# de (B,1) reshape(-1) aplana a (B,) (de matriz a vector) para recorrerlo rápido meta guarda la info (nombre, carpeta).\n",
        "#pb guarda las probabilidades (ya en un array plano). Con zip(meta, pb) para  ambos simultaneo y asignar las predicciones a cada archivo.\n",
        "\n",
        "    if X: #si el número de grupos no es multiplo del batch (16) pon tu 34=16+16+2 pues eso 2 los captura, los que sobrarian de X.\n",
        "        xb = np.stack(X, axis=0) #junta lo que queda en formato (len(X), K, H, W, 3)\n",
        "        pb = clf_seq(xb, training=False).numpy().reshape(-1) #calcula las probs y el .reshape da un vector de probabilidad por grupo.\n",
        "        for (n,c), p in zip(meta, pb): preds[c] = float(p)\n",
        "    return preds #Devuelves un diccionario con las predicciones.\n",
        "\n",
        "#en el for pb es de probs de pb = clf_seq(...).numpy().reshape(-1), zip los une para recorrerlos en paralelo, for desempaqueta(n,c) y p de prob.\n",
        "#zip para empareja cada carpeta con su probabilidad y se mete al dic de \"preds\" la clave es carpeta de frames (ruta del salto).\n",
        "#valor es la probabilidad de ser \"no válido\" (recordar que la red esta entrenada con sigmoid).\n",
        "\n",
        "# ---------- MEDIAPIPE (solo para válidos con margen) ----------\n",
        "#es la parte del postprocesado con mediapipe solo para vídeos válidos y con probabilidad clara, no en el aspectro dudoso (se definio en MARGEN_OK)\n",
        "try:\n",
        "    import mediapipe as mp #intenta instalar mediapipe, si esta instalada no pasa nada, es por seguridad.\n",
        "except ModuleNotFoundError: #si no esta instalada se lanza la excepción\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mediapipe==0.10.14\", \"--quiet\"]) #aca se usa al interprete para instalar pip.\n",
        "    import mediapipe as mp #una vez instalado importa mediapipe (est bloque es de seguridad lo puedes quitar si tienes mediapipe)\n",
        "\n",
        "mp_pose       = mp.solutions.pose #es la parte de mediapipe que sirve para el esqueleto humano (pose estimation).(se uso para analizar los saltos válidos:verificar técnica y postura.)\n",
        "FUENTE        = cv2.FONT_HERSHEY_SIMPLEX#se usa en OpenCV (cv2.putText) para los o0verlays (“VÁLIDO”, coordenadas, ángulos, etc.).\n",
        "FPS_SALIDA    = 30 #define los frames por segundo de los videos de salida que se generan al exportar con overlays.(fijos a 30 FPS).\n",
        "\n",
        "#para calcular un punto representativo del pie a partir de los landmarks que da MediaPipe Pose. (notas que las funciones que empiezan con \"_\" son como una función privada, no le menees a menos que sepas exacatamente que haces, pero asi ya funciona bien.)\n",
        "\n",
        "def _xy_pie_mediana(landmarks): #es la lista de puntos clave de mediapipe y esta en coordenadas normalizadas (x,y,z)(z es profundidad peor no en m, más respecto al cuerpo, o sea si te salen valores negativos la cámara esta adelante.)\n",
        "    cand = []\n",
        "    for lm in (mp_pose.PoseLandmark.LEFT_HEEL, #talon izquierdo.\n",
        "               mp_pose.PoseLandmark.RIGHT_HEEL, #talon derecho\n",
        "               mp_pose.PoseLandmark.LEFT_FOOT_INDEX, #punta del pie izquierdo.\n",
        "               mp_pose.PoseLandmark.RIGHT_FOOT_INDEX): #punta del pie derecho.\n",
        "        L = landmarks[lm]; cand.append((L.x, L.y)) #saca sus coordenadas (x,y) y las guarda en cand\n",
        "    xs, ys = zip(*cand); return float(np.median(xs)), float(np.median(ys)) #Se separan las coordenadas X e Y, se calcula media de x e y, tenemos un punto central robusto del pie.\n",
        "#use la media porque es menos sensible a valores atípicos, para que sea más robusto.\n",
        "\n",
        "# aca es lo del pie promedio que vimos arriba en un bucle que procesa muchos frames.\n",
        "\n",
        "def extraer_xy_de_frames(rutas_frames):\n",
        "    xs, ys = [], [] #aca se guardan todas las coordenadas Y e X del pie medio (lo de arriba) frame a frame.\n",
        "    ##pose es el modelo de MP que detecta landmarks del cuerpo, static... para secuencias de video, confianza mínima para deterctar la pose y\n",
        "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose: #confianza mínima para seguir landmarks a lo largo de un video.\n",
        "        for fp in rutas_frames: #Itera sobre la ruta de frames.\n",
        "            img = cv2.imread(fp) #carga la imagen desde disco (drive)\n",
        "            if img is None: continue #si no se puede cargar, pasa la siguiente.\n",
        "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #pasa de bgr a rgb.\n",
        "            res = pose.process(rgb) #regresa un objeto con la pose detectada ya con los landmarks detectados en ese frame.\n",
        "            if res.pose_landmarks: #revisa si MP pose encontro puntos del cuerpo, si si pues da los 33 landmarks\n",
        "                x, y = _xy_pie_mediana(res.pose_landmarks.landmark) #llama esa función que es para el pie robusto en (x,y)\n",
        "                xs.append(x); ys.append(y) #guarda los X y los Y en listas separadas.\n",
        "    return xs, ys #regresa las trayectorias. (cada (xᵢ, yᵢ) corresponde al pie en un frame.)\n",
        "\n",
        "#análogo de extraer_xy_de_frames, pero no recibe una lista de rutas (fs) de imágenes ya guardadas, recibe un único video (path_video) y va leyendo frame por frame con OpenCV.\n",
        "#fs listra completa de frames, lista de rutas a frames.\n",
        "#fp frame individual (una ruta de la lista ), o sea la ruta de un solo video.\n",
        "def extraer_xy_de_video(path_video):\n",
        "    xs, ys = [], [] #lista donde se acumulan las coordenadas X, Y de los pies. (se guarda la serie de posiciones (en tiempo) de los pies.)\n",
        "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose: # el false dice video en movimiento,optimiza el tracking usando frames previos del vídeo.\n",
        "        cap = cv2.VideoCapture(path_video) #cap es un objeto de opencv permite leer frame a frame y abre el archivo de video.\n",
        "        while True:\n",
        "            ok, frame = cap.read() #ok si leyo un frame de video y cap.read() pues lee un video. (cuando no quedan frames se corta el bucle.)\n",
        "            if not ok: break\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #pasa de BGR a RGB porque asi lo pide mediapipe.\n",
        "            res = pose.process(rgb) #analiza ese rame con el modelo de pose\n",
        "            if res.pose_landmarks:#si detecta una pose tomas los landmarks de pies con tu función _xy_pie_mediana. (pie robusto)\n",
        "                x, y = _xy_pie_mediana(res.pose_landmarks.landmark)\n",
        "                xs.append(x); ys.append(y) #guarda las coordenadas en las listas xs y ys (por separado).\n",
        "        cap.release() #ya se termino de usar pues se cierra. (también para libera memoria que usaba Opencv)\n",
        "    return xs, ys #devuelve las coordenadas que se sacan del video.\n",
        "\n",
        "#este arreglo es para eliminar ruido, picos bruscos en las coordenadas de los pies a lo largo del tiempo, para que la curva sea más suave.\n",
        "#al sacar promedio se reduce el ruido. k (ventana) es ponle que es 3, toma 3 valores seguidos, saca promedio y lo usa como valor de suavizado.\n",
        "#media movil es la tecnica de suavizado más simple(no hace falta más) tomas la ventana k d evalores seguidos, promedio, pasas de ventana y asi.\n",
        "# el ker es un kernet uniforme de longitud k, si es 3 seria [1/3, 1/3, 1/3] y sirve para sacar el promedio de cada ventana de k elementos.\n",
        "#valores de suavizado pequeño borran poco ruido (la señal es parecida) si es grande se van los picos pero la señal se hace lenta, tarda en reaccionar a cambios bruscos.\n",
        "#con k=11, cada punto se promedia con sus 10 vecinos más cercanos (5 a la izquierda y 5 a la derecha)cada punto se reemplaza por el promedio de sus k vecinos.\n",
        "\n",
        "def _suavizar(v, k=11):   # cambia a como mejor te convenga, pero yo lo hice así.(cuando hablo asi, me refiero al lector si alguien lo lee)\n",
        "    if len(v) < 3: return np.array(v, float) #si el vector v solo tiene menos de 3 valores no lo suaviza, no tiene sentido, lo regresa tal cual.\n",
        "    k = max(3, int(k) | 1); ker = np.ones(k)/k #k es el tamaño de la ventana de suavizado, el 1 para que sea impar(media móvil se centra mejor con ventanas impares) y 3l max 3 para que no sea menor que 3.\n",
        "    return np.convolve(np.asarray(v, float), ker, mode=\"same\") #np.convolve aplica el kernel sobre la señal v y mode devuelve un vector de la misma longitud que la entrada.\n",
        "\n",
        "#ys y xs son las coordenadas normalizadas(0 a 1) de los pies (pie robusto) el tau es la sensibilidad para ver si es un salto(uso el valor default). o sea que es el umbral base de la velocidad vertical para decidir cambios claros (despegue/aterrizaje).\n",
        "#luego se suaviza para trabajar con x e y ya suavizados, con media movíl y ventana 11, para eliminar ruido de Mediapipe.\n",
        "#si en y (vertical)son menos de 11puntos regresa none, si el salto es muy pequeño no detecta, no seria confiable.\n",
        "#punto más alto del pie en y es el menor porque en C normalizadas y crece para abajo. y vy ≈y[t+1] - y[t]. velocidad vertical.\n",
        "#y(t) seria como la posición del frame en t y si lo vemos como limite Δt=tiempo entre frames ( 30 fps, Δ𝑡=1/30 Δt=1/30 s).  pero como es contante y abusando muchísimo del lenguaje sería como una derivada discreta.\n",
        "#np.abs toma el valor absoluto porque solo quiero ver que tan duro se mueve, le saco la mediana, si no hay datos 0.0\n",
        "#el tau el es estandar y 0.5*med es la mitad de la mediana del movimiento real (se usa el mayor de los dos)\n",
        "#si el salto es muy suave (valores pequeños), se usa el mínimo tau.\n",
        "#Si el salto es más fuerte (valores grandes), se sube el umbral a 0.5*med.\n",
        "#o sea se calcula la mediana de la velocidad. saca la mitad 0.5*med y si la mitad de la mediana es mayor que tau, usamos 0.5 * med.\n",
        "#Si 0.5*med es menor que tau, usamos tau, para que no baje mucho el umbral y confundir ruido con salto.\n",
        "#antes toma la velocidad vertical (vy) antes del ápice(punto máximo) apex-1 para mirar solo la fase de subida. si apex es muy pequeño 0 o 1 al inicio del video apex-1 sería 0 o negativo.\n",
        "#max(apex-1, 1), para que al menos tomemos hasta el índice 1.nunca se pide una porción de tamaño 0 (da problemas al buscar candidatos).\n",
        "#cand_take busca los frames antes del ápice donde la velocidad vertical es muy negativa vy < -tau_eff el pie sube con claridad o sea que devuelve los candidatos a despegue.\n",
        "#take=cand_take si hay candidatos, toma el último ([-1]) la última vez que el despegue es fuerte, la asocio al despegue.\n",
        "#si no los hay usa un fallback heurístico apex-4 (4 frames antes del apendice acotado a ≥ 0) es el plan B si no hay despegue claro (vy < -tau_eff)\n",
        "#supone que el despegue ocurrió unos 4 frames antes del punto más alto. (el video es a 30 fps, serian de unos 0.13 segundos antes del ápice.)\n",
        "#en después es la parte de la bajada, cand... ve frames en los que la velocidad vertical es positiva (vy > tau_eff) osea que el pie está bajando con fuerza y devuelve los índices donde ocurre esa condición.\n",
        "#si hay candidatos en land=apex... ( o sea cand_land.size > 0) usa el primer(cand_land[0] el primer cruce donde la bajada es clara) y si no hay, para eso esta el +4 o sea que el aterrizaje se dio 4 frames después del apex.\n",
        "#len(x)-1 si x tiene la misma longitud que tu serie temporal (e,j100 frames), índices válidos de 0 hasta 99. Así que len(x)-1 es el último índice válido.\n",
        "#min(take, len(x)-1) asegura que take no sea mayor que el último frame. (e.j take=101 lo pasa a 99).\n",
        "#max(0, min(...))asegura que take nunca sea menor que 0 (e.j take=-6 lo pasa a 0).El resultado siempre esta entre 0 y len(x)-1. Lo mismo para land.\n",
        "#en el if es porque en un salto válido el despegue debe ocurrir antes que el aterrizaje para asegurar más.\n",
        "#take = el frame del despegue (cuando el pie se separa del suelo).\n",
        "#land = el frame del aterrizaje (cuando el pie vuelve a tocar el suelo).\n",
        "#la función devuelve un segmento de salto como una pareja de índices donde empieza y termina el salto en frames.\n",
        "\n",
        "def _segmento_salto(xs, ys, tau=0.002):\n",
        "    x = _suavizar(xs, 11); y = _suavizar(ys, 11)\n",
        "    if len(y) < 11: return None\n",
        "    apex = int(np.argmin(y)); vy = np.diff(y)\n",
        "    med = np.median(np.abs(vy)) if len(vy) else 0.0; tau_eff = max(tau, 0.5*med)\n",
        "    antes = vy[:max(apex-1,1)]; cand_take = np.where(antes < -tau_eff)[0]; take = cand_take[-1] if cand_take.size else max(apex-4, 0)\n",
        "    despues = vy[apex:];       cand_land = np.where(despues >  tau_eff)[0]; land = apex + (cand_land[0] if cand_land.size else 4)\n",
        "    take = max(0, min(take, len(x)-1)); land = max(0, min(land, len(x)-1))\n",
        "    if land <= take: return None\n",
        "    return take, land\n",
        "\n",
        "#mide que tan lejos se movio el pie durante el salto, basicamente calcula la distancia.\n",
        "\n",
        "def calcular_distancia_solo_segmento(xs, ys, escala=ESCALA_METROS): #lista de arrays con las coordenadas normalizadas del pie en cada frame y la escala son metros reales de la anchura normalizada 1.0\n",
        "    if not xs or not ys: return None #si las listas estan vacias regresa none (no se pudo calcular distancia)\n",
        "    x = _suavizar(xs, 11); seg = _segmento_salto(xs, ys) #suavizado a xs  (horizontal) Intenta detectar despegue y aterrizaje con _segmento_salto.\n",
        "    if seg is not None: #si se detecto un segmento (salto y aterrizaje) seg = (take, land)\n",
        "        i0, i1 = seg; dx = float(x[i1] - x[i0]) #i0 = índice de despegue, i1 = índice de aterrizaje y vecuánto avanzó horizontalmente el pie (dx = x[i1] - x[i0])\n",
        "        if dx <= 0: dx = float(np.max(x) - np.min(x)) #si por algo se invierte el orden o ruido (como fallback mide todo el rango horizontal (max - min).)\n",
        "    else: #Si no se detectó el segmento del salto.\n",
        "        dx = float(np.max(x) - np.min(x)) #usa la distancia horizontal como estimación (peor pero seguro).\n",
        "    return abs(dx)*float(escala) #da distancia en metros: dx es en coordenadas normalizadas [0–1].\n",
        "    #Multiplica por escala (13.20 m este valor es ajustable, prueba como funciona mejor, yo por limitaciones ya no puedo seguir probando, pero es aledaño).\n",
        "    #abs() valor positivo (valor absoluto).\n",
        "\n",
        "def calificacion(dist): #distancia del salto en metros (float) o none si no puede calificar\n",
        "    if dist is None: return \"No detectado\" #si no hay valor por lo que sea, regresa no detectado.\n",
        "    if dist < 6.0:   return \"normalito\" #si la distancia es menor a 6m, normalito.\n",
        "    if dist < 7.5:   return \"muy bien\" #menor a 7.5 muy bien (son los que más hay, lo que concuerda con la vida real)\n",
        "    if dist < 8.5:   return \"Muy Bueno\" #menor a 8.5 muy bueno. (califica 10 así, en la vida real eran 5)\n",
        "    return \"perfecto\" #si se pasa de 8.5 regresa perfecto (en la vida real hay 6 arriba del 8.5, califico 17 asi, se pasa por 11, pero es corregible con los valores que te mencione antes, no es que no sirva la red, faltan intentos.)\n",
        "\n",
        "# ---------- Heurística para decidir en banda dudosa ----------\n",
        "#El objetivo es decidir si un salto debe considerarse inválido aunque la red neuronal no esté 100% segura.\n",
        "def heuristica_invalido(xs, ys): #defino una función que recibe xs y ys\n",
        "    xs = np.asarray(xs); ys = np.asarray(ys) #pasa xs y ys en arrays de np(para usar .max(), .min(), operaciones vectorizadas y así).\n",
        "    cov = len(xs) / max(1, FRAMES_POR_VIDEO)# mide el % frames con pose (k=24 y xs tiene longitud 10, detección en 10/24 = 0.41 (41%)) el max evita 0/x si k=0.\n",
        "    dx  = (xs.max()-xs.min()) if xs.size else 0.0 #desplazamiento horizontal del pie (diferencia máximo y mínimo en X).\n",
        "    dy  = (ys.max()-ys.min()) if ys.size else 0.0 #en vertical, si(ys.size == 0), devuelve 0.0 en vez de intentar max() o min()(error).\n",
        "    flags = 0 #contador de alertas (si el% de frames detectados es bajo 1 alerta, si casi no se mueve horizontalmente o verticalmente otro y así)\n",
        "    if cov < 0.40:  flags += 1\n",
        "    if dx  < 0.020: flags += 1\n",
        "    if dy  < 0.015: flags += 1\n",
        "    return flags >= 2 #si se cumplen 2 de esas condiciones se considera inválido (heurísticamente, o sea no sirve.)\n",
        "\n",
        "# ---------- Overlays ----------\n",
        "#frames lista de rutas a imágenes (frames de un video).\n",
        "#texto es el texto que se va a escribir sobre cada frame.\n",
        "#color es el color del texto en formato BGR ((0,255,0) = verde).\n",
        "#out_path es la ruta donde se guardará el video final.\n",
        "#out=cv2... crea un escritor de video (VideoWriter). cv2.Vid...mp4v es para guardar en MP4, luego los fps de salida,(w, h) resolución del video.\n",
        "#escribe el texto en el cuadro negro Posición (40,65) (empieza el texto).\n",
        "#Fuente : FONT_HERSHEY_SIMPLEX.\n",
        "#Tamaño de letra → 1.1.\n",
        "#Color el que le pongas.\n",
        "#Grosor → 2.\n",
        "#LINE_AA para el suavizado de bordes (para que se vea mejor).\n",
        "#como dato (0,0,0) es negro y el -1 es para que se relllene de ese color.\n",
        "def overlay_frames(frames, texto, color, out_path):\n",
        "    if not GENERAR_OVERLAYS or not frames: return #si no hya frames sale sin hacer nada.\n",
        "    first = cv2.imread(frames[0]); #Carga el primer frame para conocer el ancho (w) y alto (h) de las imágenes.\n",
        "    if first is None: return\n",
        "    h, w = first.shape[:2] #para inicializar el video correctamente. (el 2 es porque no agarro coloer, solo h y w alto y ancho)\n",
        "    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), FPS_SALIDA, (w, h))\n",
        "    for fp in frames: #itera sobre todos los frames\n",
        "        img = cv2.imread(fp) #le cada imagen.\n",
        "        if img is None: continue #si no puede leer salta.\n",
        "        x2 = min(w-20, 40 + int(0.6*w)) #es para la caja de texto que saldra en el video, se llama ancho dinámico\n",
        "        cv2.rectangle(img, (30, 20), (x2, 80), (0,0,0), -1) #dibuja un rectangulo negro, en la parte superior (para que se vea el texto)\n",
        "        cv2.putText(img, texto, (40, 65), cv2.FONT_HERSHEY_SIMPLEX, 1.1, color, 2, cv2.LINE_AA)\n",
        "        out.write(img) #agrega el frame modificado (con overlay) al archivo de video.\n",
        "    out.release() #cierra el archivo de video y guarda el resultado.\n",
        "\n",
        "#w-20 es el ancho del video menos 20 píxeles y 40 + int(0.6*w) el valor que calculamos antes.\n",
        "#elige el menor de los dos, para asegurarse de que el rectángulo no se pase del borde derecho.\n",
        "#el min es para que jamas dibuje afuera del rectangulo y se estira para que se vea mejor.\n",
        "#nota :Rojo usado cuando el modelo/heurística decide que el salto es inválido. Verde es usado cuando pasa como válido y se mide la distancia.\n",
        "\n",
        "def overlay_video(in_path, texto, color, out_path): #frame a frame tipo st y escribe otro video con clasificación superpuesta (entrada, (cadenaN/NV), color, Ruta de guardado.\n",
        "    if not GENERAR_OVERLAYS: return #igual que antes si es False,no hace nada (sirve como switch para no gastar tiempo en renderizado).\n",
        "    cap = cv2.VideoCapture(in_path) #abre el archivo de video.\n",
        "    if not cap.isOpened(): return #devuelve False o sea que sale sin hacer nada.\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\"); fps = cap.get(cv2.CAP_PROP_FPS) or 30 #codec para mp4, fps del video de entrada(30), si no se detecta igual usa 30.\n",
        "    w, h = int(cap.get(3)), int(cap.get(4)) #ancho(3) y alto del video(4) (son IDs de propiedades predefinidas en OpenCV)\n",
        "    out = cv2.VideoWriter(out_path, fourcc, float(fps), (w, h)) #para guardar, ruta y nombre de salida, códec, los fps que tendra, tamaño de fotograma (ancho, alto)\n",
        "    while True: #buble para leer cada frame el video original.\n",
        "        ok, frame = cap.read() #si pudo leer fotogramas, ok (como dato los frames son matriz numpy porque uso opencv)\n",
        "        if not ok: break #se sigue leyendo frames hasta que no hay más.\n",
        "        x2 = min(w-20, 40 + int(0.6*w)) #ancho (se saco de cap.get(3)) 60% del ancho del video, le sumas un margen de 40 píxeles(el rectángulo se hace un poquito más ancho hacia la derecha)\n",
        "        cv2.rectangle(frame, (30, 20), (x2, 80), (0,0,0), -1) #dibuja un rectángulo negro arriba. (es lo mismo que le otro rectangulo)\n",
        "        cv2.putText(frame, texto, (40, 65), cv2.FONT_HERSHEY_SIMPLEX, 1.1, color, 2, cv2.LINE_AA)#escribe el texto (ej:\"Inválido (p=0.72)\") en el color que le toca.\n",
        "        out.write(frame) #agrega el frame procesado (ya con texto) al video de salida.\n",
        "    cap.release(); out.release() #cap.release(): libera el video de entrada y out.release(): guarda y cierra el video de salida.\n",
        "\n",
        "# ---------- MAIN ----------\n",
        "#aca se empiezan a alistar los grupos que se van a evaluar, es el main de inferencia.\n",
        "t0 = time.time() #como referencia para ver cuánto tardó todo el proceso de entrenado (guarda el tiempo actual o sea los segundos desde el epoch)\n",
        "grupos_val   = [(os.path.basename(p), p) for p in listar_subcarpetas(RUTA_VALIDOS)] #devuelve las subcarpetas de válidos, cada subcarpeta p es una tupla, con nombre de la carpeta y la ruta completa de la subcarpeta que resulta en: grupos_val es una lista de tuplas.\n",
        "grupos_noval = [(os.path.basename(p), p) for p in listar_subcarpetas(RUTA_NOVALIDOS)] #lo mismo lista de tuplas con (nombre, ruta) para los grupos de no válidos.\n",
        "todos = grupos_noval + grupos_val #se concatenan las listas para tener todos los videos V y NV. (se recorre para clasificación (clasificar_grupos))\n",
        "print(f\"[INFO] grupos totales: {len(todos)} (V={len(grupos_val)}, NV={len(grupos_noval)})\") #grupos totales, válidos y no válidos.\n",
        "\n",
        "# 1) clasificar todos rápido\n",
        "#llama a tu función clasificar_grupos (se definio antes). Le pasa la lista \"todos\", que contiene tuplas con (nombre, ruta) de cada grupo de frames.\n",
        "#la función hace este flujo interno: Para cada carpeta de frames: Lista las imágenes con listar_frames_en_carpeta.\n",
        "#selecciona K frames equiespaciados (_sel_k).\n",
        "#preprocesa cada frame con _stack_from_sel lo que lo convierte en tensor de forma (K, H, W, 3).\n",
        "#agrupa varios grupos hasta llegar a N_BATCH (16 por defecto) y forma un batch tensor 5D de forma (B, K, H, W, 3).\n",
        "#pasa ese batch por el modelo clf_seq para obtener las probabilidades de ser inválido (p_inv).\n",
        "#pasa el resultado de salida (un tensor de probas) a un vector NumPy 1D con .numpy().reshape(-1).\n",
        "#y ya por último va guardando los resultados en un diccionario.\n",
        "#pdict es un diccionario cuyas keys son las rutas de los grupos y sus values(datos de una variable) son las probas de ser inválidos.\n",
        "#aquí no se hace heurística, ni overlays, ni cálculos de distancia. Solo el modelo en modo batch. (mucho más eficiente que ir uno por uno)\n",
        "#Entrada: lista \"todos\" con los grupos.\n",
        "#Proceso: preprocesar, batch, modelo, proba.\n",
        "#Salida: diccionario pdict con la probabilidad de invalidez por cada carpeta de frames.\n",
        "\n",
        "pdict = clasificar_grupos(todos)\n",
        "\n",
        "#Segundo paso de la inferencia: después de obtener las probabilidades (pdict), ahora vemos qué hacer con cada salto\n",
        "#Si es inválido rojo + texto “Inválido”. Si es válido se usa MediaPipe para calcular distancia y dar calificación.\n",
        "#Aquí todavía no se decide, solo se prepara la info base (p, p_txt, out_name) para cada grupo.\n",
        "#p_txt tambien es para para superponer en overlays y para imprimir logs.\n",
        "#out_name define el nombre del archivo de salida donde se guardará el video con overlay (el marcador de válido/inválido o distancia).\n",
        "#CARPETA_RESULT es la carpeta central de resultados (/content/drive/My Drive/videoscombi/resultados/elnombredetuvideo_resultado.mp4)\n",
        "\n",
        "\n",
        "resultados = [] #un diccionario con salida amigable para leer lo que pasa ({\"grupo\": ruta, \"p_inv\": p, \"invalido\": True/False, \"distancia_m\": 7.2, \"calificacion\": \"Muy Bueno\"})\n",
        "# 2) procesar según decisión; MediaPipe solo para válidos con margen\n",
        "for nom, car in todos: #itera sobre cada grupo de \"todos\" nom:nombre de la carpeta y car=ruta completa de esa carpeta.\n",
        "    p = pdict.get(car, None)#ve en pdict (dic de probas) el valor de esa ruta car, devuelve None si no encuentra.p es la proba que ese grupo sea inválido.\n",
        "    p_txt = \"N/A\" if p is None else f\"{p:.2f}\" #pasa la probabilidad de p en texto bonito, si p=None, escribe \"N/A\". Si sí hubo,redondea a 2 decimales (0.8231 → 0.82).\n",
        "    out_name = os.path.join(CARPETA_RESULT, f\"{nom}_resultado.mp4\")\n",
        "\n",
        "    # Decisión rápida\n",
        "    #aca empieza y ala decisión binaria.\n",
        "#if p is none...:se decide si el salto es inválido: p is None: no hubo predicción, asume inválido (porque no se pudo evaluar bien).\n",
        "#p >= UMBRAL_INVALIDO: la probabilidad supera el umbral → se clasifica como inválido.\n",
        "#UMBRAL_INVALIDO es el corte o sea  0.57. Si la red cree con ≥ 57% que es inválido es inválido.\n",
        "#if GENERAR es por si se pide generar overlays (True), se crea un video con la etiqueta. listar_frames_en_carpeta(car)recoge los frames del grupo.\n",
        "#overlay_frames(..., f\"Invalido (p_inv={p_txt})\",...)dibuja en cada frame: Texto: \"Invalido (p_inv=0.85)\"\n",
        "#color: (0,0,255)rojo en formato BGR de OpenCV, guarda como archivo en out_name.\n",
        "#en resultados mete un registro a la lista de \"resultados\"con: \"grupo\": car siendo la ruta del grupo. \"p_inv\": p la proba de inválido.\n",
        "#\"invalido\":True que marcado como inválido. \"distancia_m\": None si no se mide distancia porque no tiene sentido en inválidos.\n",
        "#\"calificacion\": \"Invalido\" → etiqueta textual.\n",
        "# la decisión: Si un salto es claramente inválido (o ni siquiera se pudo evaluar bien) se marca y no gastas más tiempo.\n",
        "#Solo los que pasan esta condición siguen en el bucle para cálculo de distancia y calificación.\n",
        "\n",
        "\n",
        "    if (p is None) or (p >= UMBRAL_INVALIDO):\n",
        "        if GENERAR_OVERLAYS:\n",
        "            overlay_frames(listar_frames_en_carpeta(car), f\"Invalido (p_inv={p_txt})\", (0,0,255), out_name)\n",
        "        resultados.append({\"grupo\": car, \"p_inv\": p, \"invalido\": True,\n",
        "                           \"distancia_m\": None, \"calificacion\": \"Invalido\"})\n",
        "        print(f\"{nom}: Invalido (p_inv={p_txt})\") #log en consola.\n",
        "        continue #pasa al siguiente grupo sin ejecutar todo el bucle. o sea que si es inválido el grupo, no pasa a MediaPipe ni a calcular distancias.\n",
        "\n",
        "    # Banda dudosa correr pose y decidir con heurística\n",
        "#recordar que UMBRAL_INVALIDO = 0.57 y MARGEN_OK = 0.04. Por eso cualquier predicción entre 0.53 y 0.57 entra en esta condición.\n",
        "#la idea es que si la probabilidad está cerca del umbral, no decides solo con la red, sino que aplicas un chequeo adicional.\n",
        "#si existe un video (vd no está vacío) se lo analiza con extraer_xy_de_video.\n",
        "#Si no hay video pero sí frames se usas extraer_xy_de_frames.\n",
        "#Resultado: xs = coordenadas horizontales (x) del pie a lo largo de los frames. ys = coordenadas verticales (y) del pie a lo largo de los frames.\n",
        "#ifheuristica...: Aplicas la heurística (que hicimos): Cobertura muy baja de frames con pose.\n",
        "#Movimiento horizontal (dx) muy pequeño.\n",
        "#Movimiento vertical (dy) muy pequeño. Si al menos 2 condiciones se cumplen se marca como inválido.\n",
        "#es como un  mini juez extra: Si la red duda, se usa MediaPipe para observar el movimiento del pie.\n",
        "#Si el movimiento es bajo, se fuerza la etiqueta de inválido. Si el movimiento sí parece salto, se le da chance de medirse como válido.\n",
        "\n",
        "    if p >= UMBRAL_INVALIDO - MARGEN_OK:\n",
        "        fr = listar_frames_en_carpeta(car); vd = listar_videos(car) #carga los frames del video completo del grupo y prepara los datos para MPipe y sacar las coordenadas del pie.\n",
        "        xs, ys = (extraer_xy_de_video(vd[0]) if vd else extraer_xy_de_frames(fr))\n",
        "        if heuristica_invalido(xs, ys):\n",
        "            resultados.append({\"grupo\": car, \"p_inv\": p, \"invalido\": True,  #se agrega un registro de listado\n",
        "                               \"distancia_m\": None, \"calificacion\": \"Invalido\"})#\"invalido\" es True \"distancia_m\": None porque no se mide distancia en inválidos y \"calificacion\": \"Invalido\".\n",
        "            print(f\"{nom}: Invalido (p_inv={p_txt}, heurística)\") #imprime algo asi : nombredelsalto(depende como lo guardaste): Invalido (p_inv=0.55, heurística)\n",
        "            continue\n",
        "#si la heurística confirma que es inválido se acaba, ya no procesa ese grupo.\n",
        "#si NO lo ve inválido, seguimos a medir distancia como válido (con margen dudoso)\n",
        "\n",
        "    # Válido con margen => medir distancia\n",
        "    #para que no te confndas con fs y fp te voy a explicar que es fr y vd.\n",
        "    #fr: lista de rutas a imágenes (frames) dentro de la carpeta del grupo car.\n",
        "    #vd: lista de videos encontrados en la misma carpeta. Usamos ambos porque el pipeline acepta desde frames sueltos hasta video por carpeta.\n",
        "# en xs, ys: si hay video (vd no está vacío), usa el primero vd[0]. Si no, procesa la secuencia de frames fr.\n",
        "# recordar que xs = posiciones horizontales (0 izquierda y 1 derecha). ys = posiciones verticales (0 arriba y 1 abajo).\n",
        "#se calcula la distancia del salo con: el segmento del salto detectado (despegue, aterrizaje) si existe.\n",
        "#Fallback: rango horizontal si no se pudo segmentar. ESCALA_METROS (13.2) convierte del rango normalizado (0–1) a metros reales.\n",
        "#if vd y elif fr: Si hay video original escribe sobre el video (overlay_video).\n",
        "#si hay solo frames se compone un video desde las imágenes (overlay_frames).\n",
        "#nota importante: esto solo ocurre si GENERAR_OVERLAYS == True (las funciones mismas respetan ese switch).\n",
        "\n",
        "    fr = listar_frames_en_carpeta(car); vd = listar_videos(car)\n",
        "    xs, ys = (extraer_xy_de_video(vd[0]) if vd else extraer_xy_de_frames(fr)) #Extrae la trayectoria del pie (normalizadas) en el  tiempo.\n",
        "    dist = calcular_distancia_solo_segmento(xs, ys, ESCALA_METROS)#calcula la distancia horizontal del salto en m\n",
        "    nota = calificacion(dist) #como si traduciera la distancia en una etiqueta cualitativa.\n",
        "    texto = \"Sin coordenadas\" if dist is None else f\"{dist:.2f} m • {nota} (p_inv={p_txt})\" #muestra metros con 2 decimales, la nota, y la probabilidad p_inv estimada por el modelo (p_txt, ya formateada antes) y si no hay dice que no hubo.\n",
        "    if vd: overlay_video(vd[0], texto, (0,255,0), out_name) #genera el video de salida con overlay en verde (0,255,0):\n",
        "    elif fr: overlay_frames(fr, texto, (0,255,0), out_name)\n",
        "\n",
        "#Guarda el registro para el CSV: grupo: ruta de la carpeta del salto.\n",
        "#p_inv: probabilidad de inválido (float o None).\n",
        "#invalido: False (estamos en rama de válido).\n",
        "#distancia_m: None si no hubo coordenadas; si sí, la distancia redondeada a 2 decimales.\n",
        "#calificacion: “No detectado” o la nota (“normalito”, “muy bien” y asi).\n",
        "\n",
        "    resultados.append({\"grupo\": car, \"p_inv\": p, \"invalido\": False,\n",
        "                       \"distancia_m\": None if dist is None else round(dist, 2),\n",
        "                       \"calificacion\": \"No detectado\" if dist is None else nota})\n",
        "\n",
        "    if dist is None: #logs en consola.\n",
        "        print(f\"{nom}: Sin coordenadas (p_inv={p_txt})\") #sin coordenadas:te avisa para revisar ese grupo (quizá MediaPipe falló).\n",
        "    else:\n",
        "        print(f\"{nom}: {dist:.2f} m • {nota} (p_inv={p_txt})\") #con distancia:imprime la métrica, calificación y probabilidad del modelo.\n",
        "\n",
        "#resumen:extrae la trayectoria del pie con MediaPipe (video o frames).\n",
        "#mide la distancia del salto (en metros) con el escale factor.\n",
        "#etiqueta con una calificación textual y hace overlay en verde si está habilitado.\n",
        "#Registra el resultado en la lista resultados y loggea.\n",
        "\n",
        "# CSV\n",
        "#es el cierre de todo el pipeline, se convierten los resultados en un CSV\n",
        "#df esun dataframe de pandas y la fuente es la lista \"resultados\" que se fue llenando durante el loop principal.\n",
        "#cada elemento de \"resultados\" es un diccionario que tiene: \"grupo\":carpeta o ruta del salto.\n",
        "#\"p_inv\":probabilidad estimada de inválido (float o None).\n",
        "#\"invalido\":True si es inválido, False si es válido.\n",
        "#\"distancia_m\":distancia medida en metros (None si no hubo coordenadas) y \"calificacion\": que es la etiqueta textual (\"Invalido\", \"Muy bueno\", etc.).\n",
        "#index=False evita que se guarde la columna numérica extra de índice (0,1,2,...) queda un archivo normal, que se puede abrir en Excel y cualquier editor de texto.\n",
        "#el ultimo print mide y muestra el tiempo total de ejecución desde que guarde t0 = time.time().\n",
        "#time.time()-t0 son los segundos transcurridos, formateados con 1 decimal.\n",
        "#BATCH={N_BATCH} es el tamaño de lote se uso para clasificar (yo 16, cambialo si quieres, ya te explique ventajas y desventajas antes).\n",
        "#OVERLAYS=on/off si se estaban generando videos con overlays o no (GENERAR_OVERLAYS).\n",
        "\n",
        "df = pd.DataFrame(resultados, columns=[\"grupo\",\"p_inv\",\"invalido\",\"distancia_m\",\"calificacion\"]) #columns es para que salgan en ese orden.\n",
        "df.to_csv(RUTA_CSV, index=False) #Guarda el dataframe en un archivo CSV (en tu disco o drive en mi caso es drive)\n",
        "print(f\"\\n[OK] CSV: {RUTA_CSV}\")#mensaje de confirmación en consola. (para estar seguros).\n",
        "print(f\"[TIME] Total: {time.time()-t0:.1f}s  | BATCH={N_BATCH} | OVERLAYS={'on' if GENERAR_OVERLAYS else 'off'}\")\n",
        "\n",
        "# (Opcional) Métricas rápidas si el path revela la clase:\n",
        "#por mucho que te diga que es opcional lo recomiendo ampliamente porque este sirve como evaluación rápida del modelo o sea, no recorre todo TensorFlow, solo mira el CSV de resultados.\n",
        "#if len(df)solo se ejecuta si el DataFrame df no está vacío.Esto depende de que la ruta del grupo (\"grupo\") tenga en su nombre si es válido o no válido (o sea, /videosnovalidos/ en mi caso, pero tu mira como se llaman las rutas de tus videos).\n",
        "#df[\"y\"] crea una columna nueva \"y\" con la etiqueta real de la clase: Convierte grupo a string. Marca 1 si el path contiene /videosnovalidos/.\n",
        "#Marca 0 si no lo contiene (válido). Esto es como un hack porque se usa la carpeta de origen como un ground truth.(o sea frames ya verificados por alguien, se sabe que son correctos)\n",
        "#df[\"p\"] crea la columna \"p\" con las probabilidades predichas. Si p_inv es NaN (no se pudo procesar), se rellena con -1.\n",
        "#el prec, rec, f1 son metricas clasicas de clasificación binaria (Precisión, Recall (sensiblidad), F1 Score)\n",
        "#la precisión contesta de todos los predichos como inválidos, ¿cuántos eran realmente inválidos? \"1e-9\" para evitar dividir por 0.\n",
        "#el recall es V que detecto como NV. De los casos no válidos, ¿cuántos encontró el modelo?\n",
        "#f1 score es una medida de equilibrio entre presición y recall o sea si la precisión es alta pero el recall bajo (o viceversa), el F1 score refleja ese desbalance. Toma valores entre 0 y 1, donde 1 es perfecto balance.\n",
        "#precisión alta y recall bajo:el modelo solo marca como NV cuando está muy seguro, pero deja escapar muchos NV.\n",
        "#recall alto y precisión baja:el modelo detecta casi todos los NV, pero también marca muchos V como NV.\n",
        "#f1 score: mide qué tan buen balance se logra entre ambas situaciones. (se les llama \"matriz de confución\" a los tp, fpl tn, fn)\n",
        "#resumen: este bloque es una validación express, qué tan bien trabaja el modelo al clasificar “inválidos” vs “válidos”\n",
        "#es rapido porque esta usando la carpeta de origen como etiqueta real(ground truth).\n",
        "\n",
        "if len(df):\n",
        "    df[\"y\"] = df[\"grupo\"].astype(str).str.contains(\"/videosnovalidos/\").astype(int)\n",
        "    df[\"p\"] = df[\"p_inv\"].fillna(-1) #calcula las predicciones binarias (1 noválido, 0 válido)\n",
        "    pr = (df[\"p\"] >= UMBRAL_INVALIDO).astype(int)\n",
        "    tp = int(((pr==1)&(df.y==1)).sum()) #tp (True Positive): predijo inválido y era inválido.\n",
        "    fp = int(((pr==1)&(df.y==0)).sum()) #fp (False Positive): predijo inválido pero era válido.\n",
        "    tn = int(((pr==0)&(df.y==0)).sum()) #tn (True Negative): predijo válido y era válido.\n",
        "    fn = int(((pr==0)&(df.y==1)).sum()) #fn (False Negative): predijo válido pero era inválido.\n",
        "    prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9); f1 = 2*prec*rec/(prec+rec+1e-9)\n",
        "    print(f\"\\n[MÉTRICAS] (clase=Inválido)\\nTP={tp} FP={fp} TN={tn} FN={fn}\\nPrecisión={prec:.3f}  Recall={rec:.3f}  F1={f1:.3f}\") #muestra la matriz de confusión y las métricas en 3 decimales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yNc_Eu6haj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03600e0f-4e89-485d-8ec1-9d0d2056dd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEAD(10):\n",
            "                                               grupo     p_inv  invalido  \\\n",
            "0  /content/drive/My Drive/videoscombi/videosnova...  0.695404      True   \n",
            "1  /content/drive/My Drive/videoscombi/videosnova...  0.275340     False   \n",
            "2  /content/drive/My Drive/videoscombi/videosnova...  0.441275     False   \n",
            "3  /content/drive/My Drive/videoscombi/videosnova...  0.393677     False   \n",
            "4  /content/drive/My Drive/videoscombi/videosnova...  0.155577     False   \n",
            "5  /content/drive/My Drive/videoscombi/videosnova...  0.810554      True   \n",
            "6  /content/drive/My Drive/videoscombi/videosnova...  0.347080     False   \n",
            "7  /content/drive/My Drive/videoscombi/videosnova...  0.817027      True   \n",
            "8  /content/drive/My Drive/videoscombi/videosnova...  0.794558      True   \n",
            "9  /content/drive/My Drive/videoscombi/videosnova...  0.465545     False   \n",
            "\n",
            "   distancia_m calificacion  gt  \n",
            "0          NaN     Invalido   1  \n",
            "1         6.56     muy bien   1  \n",
            "2         8.24    Muy Bueno   1  \n",
            "3        10.52     perfecto   1  \n",
            "4         7.47     muy bien   1  \n",
            "5          NaN     Invalido   1  \n",
            "6         6.87     muy bien   1  \n",
            "7          NaN     Invalido   1  \n",
            "8          NaN     Invalido   1  \n",
            "9         5.65    normalito   1  \n",
            "\n",
            "Conteo por calificación:\n",
            " calificacion\n",
            "muy bien        43\n",
            "perfecto        17\n",
            "Invalido        16\n",
            "Muy Bueno       10\n",
            "Revisar          9\n",
            "normalito        6\n",
            "No detectado     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[METRICAS] (clase=Inválido)\n",
            "TP=11 FP=5 TN=78 FN=8\n",
            "Precision=0.687  Recall=0.579  F1=0.629\n",
            "\n",
            "[CASOS DUDOSOS] (más cerca del umbral=0.60)\n",
            "                                                                                                                                       grupo    p_inv  invalido  gt  distancia_m calificacion   _delta\n",
            "/content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-30-44_result 0.598513     False   0          NaN      Revisar 0.001487\n",
            "       /content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-17-44 0.597381     False   0          NaN      Revisar 0.002619\n",
            "       /content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-53-33 0.591803     False   0          NaN      Revisar 0.008197\n",
            "       /content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-16-32 0.614720      True   0          NaN     Invalido 0.014720\n",
            "/content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-16-32_result 0.567900     False   0          NaN      Revisar 0.032100\n",
            "       /content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-55-16 0.556494     False   0          NaN      Revisar 0.043506\n",
            "                                                                                    /content/drive/My Drive/videoscombi/videosnovalidos/vn18 0.554989     False   1          NaN      Revisar 0.045011\n",
            "       /content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-30-44 0.645303      True   0          NaN     Invalido 0.045303\n",
            "       /content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-52-53 0.652934      True   0          NaN     Invalido 0.052934\n",
            "/content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 15-52-53_result 0.542894     False   0          NaN      Revisar 0.057106\n",
            "/content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-08-06_result 0.664912      True   0          NaN     Invalido 0.064912\n",
            "/content/drive/My Drive/videoscombi/videosvalidos/Men's Long Jump Final _ Tokyo Replays - YouTube - Google Chrome 2025-01-03 17-17-44_result 0.532798     False   0          NaN      Revisar 0.067202\n",
            "\n",
            "[STATS distancia_m] count=76, mean=7.38, std=1.43\n",
            "Quantiles:\n",
            " 0.00     0.36\n",
            "0.25     6.56\n",
            "0.50     7.22\n",
            "0.75     8.20\n",
            "1.00    10.52\n",
            "Name: distancia_m, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import os, re, numpy as np, pandas as pd\n",
        "#os para archivos, re para limpiar y parsear datos, np operaciones numéricas y pd tablas y leer CSV\n",
        "\n",
        "# --- rutas y umbral (usa el de la sesión si existe) ---\n",
        "RUTA = \"/content/drive/My Drive/reporte_saltos.csv\" #define la ruta del CSV que se generó en la inferencia.\n",
        "try:\n",
        "    UMBRAL_INVALIDO #intenta usar este valor (ya existe desde inferencia)\n",
        "except NameError: #si no existe da el name error\n",
        "    UMBRAL_INVALIDO = 0.60  # lo define en .60 si no existe.\n",
        "#esta celda es como portable o sea que funciona con o sin tener la celda previa ejecutada.\n",
        "\n",
        "if not os.path.exists(RUTA): #comprueba si el csv existe y si sí lo carga a df.\n",
        "    print(\"No existe el CSV:\", RUTA) #si no imprime un aviso y no intenta leer.\n",
        "else:\n",
        "    df = pd.read_csv(RUTA) #Si existe, lo lee en un DataFrame df.\n",
        "#de aquí en adelante, df contendrá las columnas que genere en el pipeline(grupo, p_inv, invalido, distancia_m, calificacion) y las que luego agregue o tú lector).\n",
        "\n",
        "    # --- limpieza de tipos ---\n",
        "#p_inv y distancia_m se convierten a floats (NaN si no se puede). invalido se normaliza a boolean (True/False).\n",
        "#para estar seguro de que el dataframe df este en un estado coherente para el análisis posterior (métricas, conteos, comparaciones)\n",
        "\n",
        "    if \"p_inv\" in df.columns: #devuelve la lista de nombres de columnas en el dataframe y asegurar la columna p_inv sea de tipo numérico (float).\n",
        "        df[\"p_inv\"] = pd.to_numeric(df[\"p_inv\"], errors=\"coerce\") #eso con pd... y errors... las cosas no convertibles(texto, NaN, - cosas asi) son NaN. (Not a Number, el valor nulo numérico en pandas).\n",
        "    if \"distancia_m\" in df.columns: #hace lo mismo pero para la columna de distancia en metros.\n",
        "        df[\"distancia_m\"] = pd.to_numeric(df[\"distancia_m\"], errors=\"coerce\")\n",
        "    if \"invalido\" in df.columns: #asegura que la columna \"inválido\" exista\n",
        "        df[\"invalido\"] = (df[\"invalido\"].astype(str).str.lower() #pasa los valores a strings y a minúsculas\n",
        "                          .map({\"true\":True, \"false\":False, \"1\":True, \"0\":False})) #map pasa los strings a boleanos (T o F) si sale algo distinto pasa a NaN.\n",
        "\n",
        "    # --- inferir ground truth (0 válido / 1 no válido) desde la ruta del grupo ---\n",
        "#la ultima linea de este bloque aplica la función infer_gt sobre la columna \"grupo\" del DataFrame df.\n",
        "#Resultado: crea una nueva columna \"gt\" (ground truth).\n",
        "#Si la columna \"grupo\" no existe, simplemente rellena \"gt\" con NaN.\n",
        "\n",
        "    def infer_gt(path):#es un ground truth en toda regla, se fija de donde salen los videos y asi los califica para no tener que hacer todo de nuevo.\n",
        "        s = str(path).lower() #pasa a minúsculas.\n",
        "        if \"videosnovalidos\" in s: return 1 #si la ruta esta en la carpeta videosnovalidos, el salto es inválido o sea etiqueta 1.\n",
        "        if \"videosvalidos\"  in s: return 0 #lo mismo pero para válidos y etiqueta 0.\n",
        "        return np.nan #si no se encuentra ninguna de esas palabras, no se puede deducir (NaN).\n",
        "\n",
        "#aplica la función infer_gt sobre la columna \"grupo\" del DataFrame df, eso crea una nueva columna \"gt\" (ground truth).\n",
        "#Si la columna \"grupo\" no existe, rellena \"gt\" con NaN.\n",
        "\n",
        "    df[\"gt\"] = df[\"grupo\"].apply(infer_gt) if \"grupo\" in df.columns else np.nan\n",
        "\n",
        "\n",
        "    # --- impresión básica ---\n",
        "    print(\"HEAD(10):\")\n",
        "    print(df.head(10)) #devuelve las primeras 10 filas del DataFrame df.\n",
        "    if \"calificacion\" in df.columns: #verifica si la columna \"calificacion\" existe en el DataFrame.\n",
        "        print(\"\\nConteo por calificación:\\n\", df[\"calificacion\"].value_counts(dropna=False)) #dropna=False para que también se cuenten los valores nulos (NaN).\n",
        "\n",
        "    # --- métricas si hay ground truth disponible ---\n",
        "#df...any() devuelve True si hay al menos un valor no nulo en esa columna.\n",
        "#has_gt será True solo si la columna gt existe y tiene algún valor válido (no todos son NaN).\n",
        "#las metricas solo se van a calcular si hay ground truth (gt) y si existe la columna invalido (predicciones del modelo).\n",
        "#sub=... crea un DataFrame sub eliminando cualquier fila donde gt o invalido sea NaN.\n",
        "#para asegura que solo trabajamos con filas con comparación válida. .copy() garantiza que el DataFrame es separado y advertencias de SettingWithCopy en pandas.\n",
        "#calcula la matriz de confusión (TP, FP, TN, FN) y las métricas estándar de clasificación (Precisión, Recall, F1).\n",
        "#para evaluar qué tan bien el sistema detecta saltos inválidos frente al ground truth.\n",
        "\n",
        "    has_gt = df[\"gt\"].notna().any() if \"gt\" in df.columns else False #se revisa si en df hay una columna gt (ground truth)\n",
        "    if has_gt and \"invalido\" in df.columns:\n",
        "        sub = df.dropna(subset=[\"gt\", \"invalido\"]).copy() #dropna para haceer un subconjunto.copy(),sub independiente de df para modificar gt con lo de abajo.\n",
        "        sub[\"gt\"] = sub[\"gt\"].astype(int) #convierte la columna gt a enteros (recordar que 0 para válido, 1 para inválido).\n",
        "        tp = int(((sub[\"invalido\"]==True)  & (sub[\"gt\"]==1)).sum()) #True Positive(tp):casos donde el modelo predijo invalido=True y el ground truth también era 1 (inválido).\n",
        "        fp = int(((sub[\"invalido\"]==True)  & (sub[\"gt\"]==0)).sum()) #False Positive(fp):el modelo dijo inválido(True)pero el gt era válido (0).\n",
        "        tn = int(((sub[\"invalido\"]==False) & (sub[\"gt\"]==0)).sum()) #True Negative(tn):el modelo dijo válido (False) y el gt era válido (0).\n",
        "        fn = int(((sub[\"invalido\"]==False) & (sub[\"gt\"]==1)).sum()) #False Negative(fn):el modelo dijo válido (False), pero en realidad era inválido (1)\n",
        "        prec = tp / (tp + fp + 1e-9) #Precisión:de todos los que el modelo marcó como inválidos, ¿qué porcentaje realmente eran inválidos? es lo mismo de la última vez.\n",
        "        rec  = tp / (tp + fn + 1e-9) #Recall (Sensibilidad o Tasa Verdaderos Positivos):de todos los inválidos reales, ¿cuántos detectó el modelo?\n",
        "        f1   = 2*prec*rec/(prec+rec + 1e-9) #F1-score:media entre precisión y recall. Sirve como balance cuando quieres considerar ambas métricas.\n",
        "        print(\"\\n[METRICAS] (clase=Inválido)\") #imprime la matriz de confusión y las métricas redondeadas a 3 decimales.\n",
        "        print(f\"TP={tp} FP={fp} TN={tn} FN={fn}\")\n",
        "        print(f\"Precision={prec:.3f}  Recall={rec:.3f}  F1={f1:.3f}\")\n",
        "\n",
        "    # --- casos más dudosos (p_inv cerca del umbral) ---\n",
        "#se crea una nueva columna _delta(auxiliar más que privada)\n",
        "#. _delta = distancia absoluta entre la probabilidad p_inv y el umbral.\n",
        "#si _delta es pequeña, significa que el modelo estuvo muy cerca de la frontera entre válido/inválido y eso es un caso dudoso.\n",
        "#en dudosos se elimina filas donde p_inv es NaN, ordena el DataFrame de menor a mayor en _delta, primero aparecerán los más ambiguos.\n",
        "#head(12) para que se muestren los 12 casos más cercanos al umbral.\n",
        "#cols_show incluye solo los que realmente existan en el DataFrame:grupo es nombre o carpeta del salto.\n",
        "#p_inv es la probabilidad de inválido. invalido es la predicción del sistema (True/False).\n",
        "#gt es el ground truth (si existe). distancia_m es la distancia estimada del salto. calificacion es texto con nota.\n",
        "#_delta es qué tan lejos del umbral quedó. c es la variable de iteración, la primera es el ekemento que se va agregando y la segunda es condición, solo incluye c si la columna existe.\n",
        "#Este bloque es para revisar los saltos más ambiguos o sea aquellos donde el modelo estuvo indeciso entre válido/inválido.\n",
        "#es útil para auditoría (revisar casos dudosos), debugging (desarrollar o ajusta el sistema pero para ver si tiene lógica) y decidir si ajustar el umbral.\n",
        "\n",
        "    if \"p_inv\" in df.columns: #verifica que existe la columna p_inv (probabilidad de inválido). Si no existe, el bloque no se ejecuta.\n",
        "        df[\"_delta\"] = (df[\"p_inv\"] - float(UMBRAL_INVALIDO)).abs()\n",
        "        dudosos = df.dropna(subset=[\"p_inv\"]).sort_values(\"_delta\").head(12)\n",
        "        cols_show = [c for c in [\"grupo\",\"p_inv\",\"invalido\",\"gt\",\"distancia_m\",\"calificacion\",\"_delta\"] if c in dudosos.columns]\n",
        "        print(f\"\\n[CASOS DUDOSOS] (más cerca del umbral={UMBRAL_INVALIDO:.2f})\") #print a 2 decimales.\n",
        "        print(dudosos[cols_show].to_string(index=False)) #agarra del dataframe \"dudosos\" solo las columnas definidas en cols_show.\n",
        "#convierte ese dataframe en una cadena bonita para imprimir, sin índice.\n",
        "\n",
        "    # --- stats de distancias (si existen) ---\n",
        "#count es el número de distancias válidas. mean es la media (promedio). std es la desviación estándar.dan una idea rápida del tamaño de la muestra\n",
        "#el promedio de saltos y qué tanto varían.\n",
        "\n",
        "\n",
        "    if \"distancia_m\" in df.columns: #se asegura que el DataFrame tenga una columna llamada distancia_m.es donde guarde la distancia en metros.\n",
        "        dist_ok = df[\"distancia_m\"].dropna() #crea una serie dist_ok que tiene distancias válidas (ignora saltos donde no se pudo calcular nada).\n",
        "        if not dist_ok.empty: #antes de calcular estadísticas, se asegura de que haya al menos un valor válido.\n",
        "            q = dist_ok.quantile([0,0.25,0.5,0.75,1.0]).round(2) #calcula cuantiles (límite estadístico)\n",
        "            print(\"\\n[STATS distancia_m] count={}, mean={:.2f}, std={:.2f}\".format(len(dist_ok), dist_ok.mean(), dist_ok.std()))\n",
        "            print(\"Quantiles:\\n\", q) #muestra la tabla con los cuantiles para que veas cómo se distribuyen los saltos.\n",
        "\n",
        "    # (Opcional) exportar a Excel para compartir\n",
        "    # df.to_excel(\"/content/drive/My Drive/reporte_saltos.xlsx\", index=False)\n",
        "    #seria para guardar en mi drive porque estoy en colab\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}